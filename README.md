# Interdisciplinary Numerical Methods: "Hub" 18.C21/16.C21

This new MIT course (**Spring 2026**) introduces numerical methods and numerical analysis to a broad audience (assuming 18.03, 18.06, or equivalents, and some programming experience).  It is divided into two 6-unit halves:

*  **18.C21/16.C21** (first half-term “hub”): basic numerical methods, including curve fitting, root finding, numerical differentiation and integration, numerical differential equations, and floating-point arithmetic. Emphasizes the complementary concerns of accuracy and computational cost.  [Prof. Steven G. Johnson](http://math.mit.edu/~stevenj) and [Prof. Youssef Marzouk](https://aeroastro.mit.edu/people/youssef-m-marzouk/).

*   Second half-term: two options for 6-unit “spokes”

    *   [18.C21A/16.C21A — numerical methods for partial differential equations](https://github.com/Shania99/numerical_spoke_pde): finite-difference, finite-volume, and finite-element methods; boundary conditions, accuracy, and stability. [Prof. Youssef Marzouk](https://aeroastro.mit.edu/people/youssef-m-marzouk/).

    *   [18.C21B/16.C21B — large-scale linear algebra](https://github.com/mitmath/numerical_spoke_linalg): sparse matrices, iterative methods, randomized methods. [Prof. Steven G. Johnson](http://math.mit.edu/~stevenj).

Taking both the hub and any spoke will count as an 18.3xx class for math majors, similar to 18.330, and as 16.90 for AeroAstro majors. Weekly homework, *exam* TBD, and spokes will include a final project with an in-class presentation.

This repository is for the "hub" course 18.C21/16.C21.

## 18.C21/16.C21 Syllabus, Spring 2026

**Instructors**: [Prof. Steven G. Johnson](http://math.mit.edu/~stevenj) and [Prof. Youssef Marzouk](https://aeroastro.mit.edu/people/youssef-m-marzouk/).  TAs: [Andrey Bryutkin](https://math.mit.edu/directory/profile.html?pid=2582) and [Rodrigo Arrieta Candia](https://math.mit.edu/directory/profile.html?pid=2409).

**Lectures**: MWF10 in 2-142 (Feb 2 - Mar 20), slides and notes posted below.

**Homework and grading**: 5 weekly psets, due Wednesdays at midnight (Feb. 11, 18, 25; Mar. 11, 18).  Students will have oral check-ins on 3 psets (randomly selected) where they have to explain their work (pass/fail per problem). One in-class exam, March 6. For accommodations, speak with [S3](https://studentlife.mit.edu/s3) and have them contact the instructors.  Grade percentages: 40% psets, 30% check-ins, 30% exam.

* Homework assignments will require some programming — you can use either **Julia or Python** (your choice; instruction and examples will use a mix of languages).

* Submit your homework *electronically* via [Gradescope on Canvas](https://canvas.mit.edu/courses/36170) as a *PDF* containing code and results (e.g., from a Jupyter notebook) and a scan of any handwritten solutions.

* **Collaboration policy:** Talk to anyone you want to and read anything you want to, with two caveats: First, make a solid effort to solve a problem on your own before discussing it with classmates or googling. Second, no matter whom you talk to or what you read, write up the solution on your own, without having their answer in front of you (this includes ChatGPT and similar). (You can use [psetpartners.mit.edu](https://psetpartners.mit.edu/) to find problem-set partners.)

**Office Hours**: Prof. Johnson: Wednesdays at 3pm in 2-345.  Prof. Marzouk: TBD.

**Resources**: [Piazza discussion forum](https://piazza.com/mit/spring2026/18c21), [math learning center](https://math.mit.edu/learningcenter/), [TSR^2 study/resource room](https://ome.mit.edu/programs/talented-scholars-resource-room-tsr2), [pset partners](https://psetpartners.mit.edu/).

**Textbook**: No required textbook, but suggestions for further reading will be posted after each lecture.  The book [*Fundamentals of Numerical Computation* (FNC)](https://fncbook.com/) by Driscoll and Braun is **freely available online**, has examples in Julia, Python, and Matlab, and is a valuable resource.  [*Fundamentals of Engineering Numerical Analysis* (FENA)](https://www.cambridge.org/core/books/fundamentals-of-engineering-numerical-analysis/D6B6B75172AD7A5A555DC506FDDA9B99) by Moin is another useful resource ([readable online](https://www-cambridge-org.libproxy.mit.edu/core/books/fundamentals-of-engineering-numerical-analysis/D6B6B75172AD7A5A555DC506FDDA9B99) with MIT certificates).

This document is a *brief* summary of what was covered in each
lecture, along with links and suggestions for further reading.  It is
*not* a good substitute for attending lecture, but may provide a
useful study guide.

## Lecture 1 (Feb 3)

* Overview and syllabus: [slides](https://docs.google.com/presentation/d/1n6VT8jQgS0T49lmyQ_89rcplTXAR8zlxU_7c1aMqO9o/edit?usp=sharing)
* Finite-difference approximations: [Julia notebook and demo](notes/finite-differences.ipynb)

Brief overview of the huge field of numerical methods, and outline of the small portion that this course will cover. Key new concerns in numerical analysis, are (i) performance (traditionally, arithmetic counts, but now memory access often dominates) and (ii) accuracy (both floating-point roundoff errors and also convergence of intrinsic approximations in the algorithms).  In contrast, the more pure, abstract mathematics of continuity is called "analysis", and is mainly concerned with (ii) but not (i): they are happy to prove that limits converge, but don't care too much how *quickly* they converge.  Whereas traditional discrete computer science is concerned with mainly with (i) but not (ii): they care about performance and resource usage, but traditional algorithms like sorting are either right or wrong, never approximate.

As a starting example, considered the the convergence of **finite-difference approximations** to derivatives df/dx of given functions f(x), which appear in many areas of numerical analysis (such as solving differential equations) and are also closely tied to *polynomial approximation and interpolation*.   By examining the errors in the finite-difference approximation, we immediately see *two* competing sources of error: *truncation* error from the non-infinitesimal Δx, and *roundoff* error from the finite precision of the arithmetic.  Understanding these two errors will be the gateway to many other subjects in numerical methods.

**Further reading**: [FNC book: Finite differences](https://fncbook.com/finitediffs), [FENA book: chapter 2](https://www-cambridge-org.libproxy.mit.edu/core/books/fundamentals-of-engineering-numerical-analysis/numerical-differentiation-finite-differences/96E5E7ED237DF7FD71F8B0F7DCA2EF7A).  There is a lot of information online on [finite difference approximations](https://en.wikipedia.org/wiki/Finite_difference),  [these 18.303 notes](https://github.com/mitmath/18303/blob/fall16/difference-approx.pdf), or [Section 5.7 of *Numerical Recipes*](http://www.it.uom.gr/teaching/linearalgebra/NumericalRecipiesInC/c5-7.pdf).   The Julia [FiniteDifferences.jl package](https://github.com/JuliaDiff/FiniteDifferences.jl) provides lots of algorithms to compute finite-difference approximations; a particularly robust and powerful way to obtain high accuracy is to employ [Richardson extrapolation](https://github.com/JuliaDiff/FiniteDifferences.jl#richardson-extrapolation) to smaller and smaller δx.  If you make δx too small, the finite precision (#digits) of [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic) leads to [catastrophic cancellation errors](https://en.wikipedia.org/wiki/Catastrophic_cancellation).


## Lecture 2 (Feb 5)

* [Floating-point introduction](notes/Floating-Point-Intro.ipynb)
* [pset 1](psets/pset1.ipynb), due Wednesday Feb. 11 at midnight.

One of the most basic sources of computational error is that **computer arithmetic is generally inexact**, leading to [roundoff errors](https://en.wikipedia.org/wiki/Round-off_error).  The reason for this is simple: computers can only work with numbers having a **finite number of digits**, so they **cannot even store** arbitrary real numbers.  Only a _finite subset_ of the real numbers can be represented using a particular number of "bits", and the question becomes _which subset_ to store, how arithmetic on this subset is defined, and how to analyze the errors compared to theoretical exact arithmetic on real numbers.

In **floating-point** arithmetic, we store both an integer coefficient and an exponent in some base: essentially, scientific notation. This allows large dynamic range and fixed _relative_ accuracy: if fl(x) is the closest floating-point number to any real x, then |fl(x)-x| < ε|x| where ε is the _machine precision_. This makes error analysis much easier and makes algorithms mostly insensitive to overall scaling or units, but has the disadvantage that it requires specialized floating-point hardware to be fast. Nowadays, all general-purpose computers, and even many little computers like your cell phones, have floating-point units.

Went through some simple definitions and examples in Julia (see notebook above), illustrating the basic ideas and a few interesting tidbits.  In particular, we looked at **error accumulation** during long calculations (e.g. summation), as well as examples of [catastrophic cancellation](https://en.wikipedia.org/wiki/Loss_of_significance) and how it can sometimes be avoided by rearranging a calculation.

**Further reading:** [FNC book: Floating-poing numbers](https://fncbook.com/floating-point).  [Trefethen & Bau's *Numerical Linear Algebra*](https://people.maths.ox.ac.uk/trefethen/text.html), lecture 13. [What Every Computer Scientist Should Know About Floating Point Arithmetic](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.6768) (David Goldberg, ACM 1991). William Kahan, [How Java's floating-point hurts everyone everywhere](http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf) (2004): contains a nice discussion of floating-point myths and misconceptions.   A brief but useful summary can be found in [this Julia-focused floating-point overview](https://discourse.julialang.org/t/psa-floating-point-arithmetic/8678) by Prof. John Gibson. Because many programmers never learn how floating-point arithmetic actually works, there are [many common myths](https://github.com/mitmath/18335/blob/spring21/notes/fp-myths.pdf) about its behavior.   (An infamous example is `0.1 + 0.2` giving `0.30000000000000004`, which people are puzzled by so frequently it has led to a web site [https://0.30000000000000004.com/](https://0.30000000000000004.com/)!)

## Lecture 3 (Feb 6)

* [Julia notebook and demo on piecewise linear interpolation](notes/linear-interpolation.ipynb)

Discussed the important problem of **interpolating** a function $f(x)$ from a set of discrete data points, which shows up in a vast number of real problems and connects to many other areas of numerical methods (e.g. differentiation and integration).  To begin with, considered the simplest algorithm of [piecewise linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) in one dimension, with points $x$ at spacing $\Delta x$, and showed that (for a twice-differentiable function) the error (the difference between the interpolant and the true function) converges as $O(\Delta x^2)$ (second-order convergence).

In particular, we looked at the *maximum* error $\max_{x \in [a,b]} |f(x) - p(x)|$ between the true function $f(x)$ and the interpolant $p(x)$.  This is also called the ["infinity norm"](https://en.wikipedia.org/wiki/Uniform_norm) $\Vert f - p \Vert_{\infty}$ as well as a variety of other names.  (Analogously, for a column vector, the infinity norm is just the [maximum absolute value](https://mathworld.wolfram.com/L-Infinity-Norm.html) of any element.)

An alternative derivation of the convergence rate, using the Taylor series of $f(x)$ on an interval, can also be found in [the 18.C21 notes from spring 2025](https://colab.research.google.com/drive/1NyU48yqYY91h2a6sZZvr1WcFUxCiDZXS?usp=sharing).

Showed some numerical examples of piecewise linear interpolation and the convergence rate (see notebook).

Finally, expressed a piecewise linear interpolant $p(x) = \sum_k f(x_k) H_k(x)$ as a linear combination of ["hat function" or "tent function"](https://en.wikipedia.org/wiki/Triangular_function) basis functions $H_k(x)$ centered at each knot.

**Further reading:** [FNC chapter 5](https://fncbook.com/overview-4) and FENA chapter 1.  Piecewise linear interpolation (among other options) is implemented  in Python by [`numpy.interp`](https://numpy.org/doc/stable/reference/generated/numpy.interp.html#numpy.interp), and several other interpolation schemes by [`scipy.interpolate`](https://docs.scipy.org/doc/scipy-1.15.1/tutorial/interpolate.html).  Interpolation packages in Julia include [Interpolations.jl](https://github.com/JuliaMath/Interpolations.jl) and [BasicInterpolators.jl](https://github.com/markmbaum/BasicInterpolators.jl) (both of which include piecewise-linear options), [Dierckx.jl (splines)](https://github.com/JuliaMath/Dierckx.jl), and [FastChebInterp.jl (high-order polynomials)](https://github.com/JuliaMath/FastChebInterp.jl).

## Optional Julia Tutorial (Feb 6 @ 5pm in 34-101)

A basic overview of the Julia programming environment for numerical computations.   This tutorial will cover what Julia is and the basics of interaction, scalar/vector/matrix arithmetic, and plotting — just as a "fancy calculator" for now (without the "real programming" features).

* [Tutorial materials](https://github.com/mitmath/julia-mit) (and links to other resources)

If possible, try to install Julia on your laptop beforehand using the instructions at the above link.  Failing that, you can run Julia in the cloud (see instructions above).

This won't be recorded, but you can find a [video of a similar tutorial by Prof. Johnson a previous year (MIT only)](https://mit.zoom.us/rec/share/oirQFHELtxJkopybssFzml7YrudRyvIlmXjmgq4YemqmjT0P7wMGCd9ilC7SMZ_o.iBZ-UO6_ww9WjwF0?startTime=1705960822000), as well as many other tutorial videos at [julialang.org/learning](https://julialang.org/learning/).


## Lecture 4 (Feb 9)
* [Julia notebook on polynomial interpolation](polynomial-interpolation.ipynb)

One approach to generalizing piecewise-linear interpolation is to interpolate $n$ points with a polynomial $p(x)$ of degree of degree $n-1$.  This is an important technique for many applications, and the general topic of approximating functions by polynomials has vast importance in numerical analysis, but requires care if $n$ becomes much larger than 4 (cubic) or so, even for smooth functions (no noise).

A general conceptual approach is to set up a system of linear equations for the polynomial coefficients $c_i$, satisfying $p(x_k) = y_k$ for each data point $(x_k, y_k)$.  This can be expressed in matrix form $Ac = y$, where the $n \times n$ matrix $A$ is known as a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix) if you are using the usual monomial basis $p(x) = c_0 + c_1 x + c_2 x^2 + \cdots$.   One quickly runs into two difficulties, however:

1. Polynomial interpolation from **equally spaced points** (in *any* basis!) can **diverge exponentially** from the underlying "true" smooth function **in between the points** (even in exact arithmetic, with no roundoff errors!).  This is called a [Runge phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon).   **Solution 1**: Use carefully chosen non-equispaced points.  A good choice that leads to *exponentially good* polynomial approximations (for smooth functions) is the [Chebyshev nodes](https://en.wikipedia.org/wiki/Chebyshev_nodes), which are clustered near the endpoints.  **Solution 2**: use a *lower* degree ($<< n$) polynomial and perform an *approximate fit* to the given points, rather than requiring the polynomial to go through them exactly.  (More on this soon.)

2. The matrix $A$ is **nearly singular** for large $n$ in the monomial basis, so floating-point roundoff errors are exacerbated.  (We will say that it has a large [condition number](https://en.wikipedia.org/wiki/Condition_number) or is "ill-conditioned", and will define this more precisely next time.)   **Solution:** It turns out that monomials are just a poorly behaved basis for high-degree polynomials, and it is much better to use [orthogonal polynomials](https://en.wikipedia.org/wiki/Orthogonal_polynomials), most commonly [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials), as a basis — with these, people regularly go to degrees of thousands or even millions.

If you address both of these problems, high-degree polynomial approximation can be a fantastic tool for describing *smooth* functions.  If you have *noisy* data, however, you should typically use much lower-degree polynomials to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting) (trying to match the noise rather than the underlying "true" function).

**Further reading**: [FNC book, chapter 9](https://fncbook.com/polynomial).  Beware that the FENA book starts with the ["Lagrange formula"](https://en.wikipedia.org/wiki/Lagrange_polynomial) for the interpolating polynomial, but this formula is very badly behaved ("numerically unstable") for high degrees and should not be used; it is superseded by the "barycentric" Lagrange formula (see FNC book; [reviewed in much more detail by Berrut and Trefethen, 2004](https://people.maths.ox.ac.uk/trefethen/barycentric.pdf)).  The subject of polynomial interpolation is the entry point into [approximation theory](https://en.wikipedia.org/wiki/Approximation_theory); if you are interested, the [book by Trefethen](https://people.maths.ox.ac.uk/trefethen/ATAP/) and accompanying [video lectures](https://people.maths.ox.ac.uk/trefethen/atapvideos.html) are a great place to get more depth.    The [`numpy.polynomials`](https://numpy.org/doc/2.2/reference/routines.polynomials.html) module contains a variety of functions for polynomial interpolation, including with Chebyshev polynomials.  A package for multi-dimensional Chebyshev polynomial interpolation in Julia is [FastChebInterp](https://github.com/JuliaMath/FastChebInterp.jl).  A pioneering package that shows off the power of Chebyshev polynomial interpolation is [`chebfun` in Matlab](https://www.chebfun.org/), along with [related packages in other languages](https://www.chebfun.org/about/projects.html).  (This approach is taken to supercomputing extremes by the [Dedalus package](https://dedalus-project.org/) to solve partial differential equations using exponentially converging polynomial approximations.)   It turns out that if you are interpolating using Chebyshev polynomials, and you choose your points $x_k$ to be Chebyshev points (either extrema or roots of the degree $n$ Chebyshev polynomial), then you don't need to form any Vandermonde-like matrix explicitly — you can get the coefficients *much* more quickly, in $O(n \log n)$ operations, using [fast Fourier transforms (FFTs)](https://en.wikipedia.org/wiki/Fast_Fourier_transform); given these, a polynomial in the Chebyshev basis can then be evaluated at any $x$ with $O(n)$ operations using the [Clenshaw algorithm](https://en.wikipedia.org/wiki/Clenshaw_algorithm).  If you don't need the explicit coefficients, the barycentric Lagrange formula (for an arbitrary set of points) has $O(n^2)$ setup cost to obtain a set of weights from the $x_k$ — still cheaper than the $O(n^3)$ cost of solving the Vandermonde system — after which the polynomial can be evaluated cheaply ($`O(n)`$) at any point $x$ similar to the other schemes.


## Lecture 5 (Feb 11)

* polynomial interpolation notebook from lecture 4 ([above](polynomial-interpolation.ipynb)).
* [handwritten condition # notes](https://www.dropbox.com/scl/fi/m691ghxq38a6lnoayrfxz/Condition-Numbers.pdf?rlkey=3x45kla27gcve0l2qv0d76sxi&st=i7kdwkps&dl=0)
* [pset 1 solutions](psets/pset1sol.ipynb)
* [pset 2](psets/pset2.ipynb): due Wed Feb 18 at midnight

Began by talking about problem (2) from the lecture-4 summary above: even with "good" non-equispaced points, it turns out that the Vandermonde matrix becomes nearly singular for large degree $n$, causing roundoff errors to explode.  We can see this numerically in the notebook (linked above).  (The problem is that monomials are a bad basis for high-degree polynomials.)  How can we begin to quantify this phenomenon?

The goal of this lecture is to precisely define the notion of a [condition number](https://en.wikipedia.org/wiki/Condition_number), which quantifies the *sensitivity* of a function f(x) to *small perturbations* in the input.  A function that has a "large" condition number is called "ill-conditioned", and *any* algorithm to compute it may suffer inaccurate results from any sort of error (whether it be roundoff errors, input noise, or other approximations) — it doesn't mean you can't use that function, but it usually means you need to be careful (about both implementation and about interpretation of the results).

For a given function $f(x)$ (with inputs and outputs in any normed vector space), the two important quantities are:
* **absolute** condition number $\kappa_a(f, x) = \lim_{\epsilon \to 0^+} \sup_{\Vert \delta x \Vert = \epsilon} \frac{\Vert \overbrace{f(x + \delta x) - f(x)}^{\delta f} \Vert}{\Vert \delta x \Vert}$.  It is the *worst case* $\Vert \delta f \Vert / \Vert \delta x \Vert$ for any arbitrarily small input perturbation $\delta x$.   Unfortunately, if the inputs and outputs of $f$ have different scales ("units"), then $\kappa_a$ may be hard to interpret (it is "dimensionful").
* **relative** condition number $\kappa_r(f, x) = \lim_{\epsilon \to 0^+} \sup_{\Vert \delta x \Vert = \epsilon} \frac{\Vert \delta f \Vert / \Vert f(x) \Vert}{\Vert \delta x \Vert / \Vert x \Vert} = \kappa_a(f, x) \frac{\Vert  x \Vert}{\Vert f(x) \Vert}$.  This is a dimensionless quantity: it is the maximum ratio of the *relative change in f* to the *relative change in x*.   Most of the time, $\kappa_r$ is the quantity of interest.

All of these quantities involve the central concept of a [norm ‖⋯‖](https://en.wikipedia.org/wiki/Normed_vector_space), which is a way of measuring the "length" of a vector.   The most familar norm, and usually the default or implicit choice for column vectors $x \in \mathbb{R}^n$, is the [Euclidean or "L₂" norm](https://mathworld.wolfram.com/L2-Norm.html) $\sqrt{x^T x}$, but other popular norms include the [L₁ norm](https://mathworld.wolfram.com/L1-Norm.html) (the ["taxicab" norm](https://en.wikipedia.org/wiki/Taxicab_geometry)) and the [maximum norm](https://en.wikipedia.org/wiki/Chebyshev_distance) ($L_\infty$).  It turns out that all norms differ by [at most an n-dependent constant factor](https://github.com/mitmath/18335/blob/spring21/notes/norm-equivalence.pdf), so the choice of norms is mostly one of convenience if you don't care about constant factors, e.g. if you are comparing $O(\varepsilon)$ to $O(\varepsilon^2)$, but it can make a big difference in optimization/fitting (e.g. different forms of regression or regularization).

For example, looked at the condition number of summation $f(x) = \sum_i x_i$.  In the $L_1$ norm, the absolute condition number is $\kappa_a = 1$, and the relative condition number is $\kappa_r = \sum_i |x_i| / |\sum_j x_j |$, which gives $\kappa_r = 1$ when all the summands $x_i$ have the same sign, but blows up $\to \infty$ as $\sum_j x_j \to 0$ (for $x \ne 0$) — this is an "ill-conditioned" sum where you can expect a large *relative* error due to [catastrophic cancellation](https://en.wikipedia.org/wiki/Catastrophic_cancellation), similar to the examples in lectures 1 and 2.

If the function is differentiable, then the condition number simplifies even further:
* If $x$ and $f(x)$ are scalars, then $\kappa_a = |f'(a)|$ is simply the magnitude of the derivative.
* If $x \in \mathbb{R}^n$ and $f(x) \in \mathbb{R}^m$ are vectors, then $\kappa_a = \Vert J \Vert$ is the ["operator" or "induced" norm](https://mathworld.wolfram.com/OperatorNorm.html) of the [Jacobian matrix J](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) ($J_{i,j} = \partial f_i /\partial x_j$), where the induced norm $\Vert A \Vert = \sup_{x \ne 0} \frac{\Vert A x \Vert}{\Vert x \Vert}$ measures "how big" a matrix $A$ is by how much it can *stretch* a vector.  (We won't worry too much yet about how to *compute* this induced norm, but if you know the [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) it is the largest singular value of $A$.)

This leads us to our next important concept, the (relative) condition number $\kappa(A)$ *of a matrix*, and in particular we considered square invertible matrices $A$.   This is defined by first considering the function $f(x) = Ax$.  Its relative condition number, from the above definitions, is $\kappa_r(f, x) = \Vert A \Vert \frac{\Vert x \Vert}{\Vert Ax \Vert}$, which depends upon $x$.  However, if we take the *upper bound* over *all x*, i.e. the *worst x*, then we get $\kappa_r(f, x) \le \Vert A \Vert \cdot \Vert A^{-1} \Vert = \kappa(A)$, the product of the norms of $A$ and its inverse.  Note that $\kappa(A) = \kappa(A^{-1})$: this also tells us the worst case sensitivity for *solving Ax=b* (multiplying by the inverse).  (If you know the SVD, then $\kappa$ is the ratio of the largest and smallest singular values of $A$.)   The condition number is a *dimensionless* (scale-invariant) measure of **how close to singular** the matrix $A$ is.  For example, if two columns of $A$ are nearly parallel, then $\kappa(A)$ will be very large: we say that the matrix is "ill-conditioned" and you should be very careful of inaccuracies when working with it.

For example, we can now explain why the monomial basis was so bad: it is easy to see that the [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix) becomes nearly singular for large $n$, since when you take large powers the columns become nearly parallel.  Conversely, it turns out that a "rotation matrix", or more generally any [orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix), has condition number equal to **1** (in the $L_2$ norm), and correspondingly using orthogonal polynomials is a much better-behaved basis.

**Further reading:** FNC book: [problems and conditioning](https://fncbook.com/conditioning) and
[conditioning linear systems](https://fncbook.com/condition-number#index-tqii4dtlc9).  [18.06 lecture notes on conditioning of linear systems](https://nbviewer.org/github/stevengj/1806/blob/fall18/lectures/Conditioning.ipynb).   Advanced books like [*Numerical Linear Algebra* by Trefethen and Bau](https://www.stat.uchicago.edu/~lekheng/courses/309/books/Trefethen-Bau.pdf) (lecture 12) treat this subject in much more depth.  See also Trefethen, lecture 3, for more in-depth coverage of norms.  A fancy vocabulary word for a vector space with a norm (plus some technical criteria) is a [Banach space](https://en.wikipedia.org/wiki/Banach_space).

## Lecture 6 (Feb 13)

* lecture notes and computational examples [notebook](notes/least-squares.ipynb)

Introduced the topic of least-square fitting of data to curves.  As long as the fitting function is linear in the unknown coefficients c (e.g. a polynomial, or some other linear combination of basis functions), showed that minimizing the sum of the squares of the errors corresponds to minimizing the norm of the residual, i.e. the "loss function" L(c) = ‖Ac - y‖², where $A$ is a "tall" matrix whose *rows* correspond to the data points and whose *columns* correspond to the basis functions.  (This is an *overdetermined* linear system because there are more equations than unknowns: we have too few parameters to make the curve go through all the data points in general, unlike interpolation.)

It is a straightforward calculus exercise to show that ∇L = 2Aᵀ(Ac - y), which means that optimal coefficients c can be found by setting the gradient to zero, and ∇L=0 implies the "normal equations" AᵀAc = Aᵀy.   In principle, these can be solved directly, but the normal equations square the condition number of A (κ(AᵀA)=κ(A)²) so they are normally solved in a different way, typically by [QR factorization](https://en.wikipedia.org/wiki/QR_decomposition) of A (or sometimes using the [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) of A); there are typically library functions that do this for you e.g. `c = numpy.linalg.lstsq(A, y)` in Python or `c = A \ y` in Julia and Matlab.

In contrast to interpolation, least-squares fitting allows you to fit to a model (e.g. a polynomial) with many fewer parameters (e.g. coefficients) than you have data points.  This allows you to avoid Runge phenomena with equispaced points, as long as the degree is much less than the number of points.  It is also crucial in applications where the data may contain errors or noise, e.g. for fitting experimental data — one does not generally wish to "overfit" the problem by trying to fit the noisy oscillations, rather than the underlying trend.  Often, you detect overfitting by separating the data into *training* data (used for the fit) and *test* or *validation* data used to [cross-validate](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) the fit on "out-of-sample" data.

**Further reading:** [FNC book chapter 3](https://fncbook.com/overview-2).  Strang's [*Introduction to Linear Algebra*](https://math.mit.edu/~gs/linearalgebra/ila6/indexila6.html) section 4.3 and [18.06 video lecture 16](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-16-projection-matrices-and-least-squares/).  [*Linear Algebra and Learning from Data*](https://math.mit.edu/~gs/learningfromdata/) section II.2 and [18.065 OCW lecture 9](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/resources/lecture-9-four-ways-to-solve-least-squares-problems/) on ways to solve least-square problems.   There are many, many books and other materials on [linear least-squares fitting](https://en.wikipedia.org/wiki/Linear_least_squares), from many different perspectives (e.g. numerical linear algebra, statistics, machine learning…) that you can find online.  The [FastChebInterp.jl package](https://github.com/JuliaMath/FastChebInterp.jl) in Julia does least-square fitting ("regression") in the basis of Chebyshev polynomials (which avoids the ill-conditioning of the Vandermonde matrix for high-degree monomials) for you in an optimized way, including multi-dimensional fitting.  Overfitting and training/test data: [VMLS book section 13.2](https://web.stanford.edu/~boyd/vmls/vmls.pdf#page=270) and [slides p. 294](https://web.stanford.edu/~boyd/vmls/vmls-slides.pdf#page=294).

## Lecture 7 (Feb 17

* Radial basis function (RBF) interpolation: [slides](https://docs.google.com/presentation/d/1NnMCzkM3LhFooB7VIDBIr4ZsvOhhRaABmbb0-9vvCd4/edit?usp=sharing)

To begin with, briefly touched an another popular method for multidimensional interpolation: [radial basis functions](https://en.wikipedia.org/wiki/Radial_basis_function). This can be understood in the same computational framework as polynomial interpolation: we form a "Vandermonde" matrix equation Ac=b, where the rows are data points and the columns are basis functions, to solve for the coefficients c.  In this case, the basis functions are $\Phi(\Vert x - x_k\Vert)$: a function $\Phi$ centered at every data point $x_k$, depending only on the radius from that point.  (Often decaying with radius, but not always.)  Usually there is a "hyperparameter" of a lengthscale in $\Phi$ (e.g. a localization lengthscale) that one has to set (e.g. heuristically, or by checking the fit against separate ["validation data"](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)).  Often, one *augments* the RBFs with *additional* basis functions, such as low-degree polynomials, leading to an [underdetermined system](https://en.wikipedia.org/wiki/Underdetermined_system) that one solves by imposing additional criteria (e.g. orthogonality between RBF coefficients and the polynomial terms, or perhaps minimizing the magnitude of the coefficients in some norm).)

Next, discussed the computational cost of interpolation and fitting.
* Solving the least-squares problem $\min_x \Vert Ax - b\Vert$ for an $m \times n$ matrix $A$ (m points and n basis functions) has $O(mn^2)$ cost, whether you use the normal equations $A^T A x = A^T b$ (which squares the condition number) or a more accurate method like QR factorization.
* Interpolation can be thought of as the special case $m=n$: solving an $n \times n$ linear system $Ax =b$ is $O(n^3)$ (usually done by [LU factorization](https://en.wikipedia.org/wiki/LU_decomposition), which is the matrix-factor form of Gaussian elimination).
* However, for specific cases there are more efficient algorithms: for **polynomial** interpolation from arbitrary points, the barycentric Lagrange formula (mentioned above) has $O(n^2)$ cost to compute the weights and $O(n)$ to evaluatge, the barycentric Lagrange formula for Chebyshev points has $O(n)$ cost (via a special formula for the weights), and using the Chebyshev-polynomial basis from Chebyshev points has $O(n \log n)$ cost to compute the coefficients (via FFTs) and $O(n)$ cost to evaluate (via a [Clenshaw recurrence](https://en.wikipedia.org/wiki/Clenshaw_algorithm)).

Briefly described the [barycentric Lagrange formula](https://en.wikipedia.org/wiki/Lagrange_polynomial#Barycentric_form) $p(x) = \frac{\sum_{j=0}^n \frac{w_j}{x - x_j} y_j}{\sum_{i=0}^n \frac{w_i}{x - x_i}}$ for weights $w_j = \prod_{k\ne j}\frac{1}{x_k - x_j}$.  Computing the $w_j$ has $O(n^2)$ cost, but for Chebyshev points $x_j = \cos(j\pi / n)$ it turns out that they have a closed-form expression $w_j = (-1)^j \begin{cases} 1/2 & j \in \{0,n\} \\ 1 & \text{otherwise} \end{cases}$ [(Trefethen, 2004)](https://people.maths.ox.ac.uk/trefethen/barycentric.pdf).  Once you have the weights $w_j$, computing $p(x)$ for any $x$ is $O(n)$, and this avoids all of the problems with the monomial basis (monomial coefficients are not even computed).   $p(x)$ here doesn't even look like a polynomial at first glance, but it turns out to be simple to derive from the Lagrange formula by "dividing by 1" (but avoids the numerical instability of the original Lagrange formula).  The key point is that we can compute and evaluate interpolating polynomials *much* more quickly and accurately than using a Vandermonde matrix.

The computational scaling becomes even more important when you go to higher dimensions: [multivariate interpolation](https://en.wikipedia.org/wiki/Multivariate_interpolation) and fitting.  We discussed a few cases cases, using two dimensions (2d) for simplicity:
* If you have a general basis $b_k(x,y)$ of 2d basis functions, you can form a Vandermonde-like matrix $A$ as above (whose columns are the basis functions and whose rows are the grid points), and solve it as before.  The matrices get much larger in higher dimensions, though!
* If you can choose a Cartesian "grid" of points, also known as a "tensor product grid" (a grid in x by a grid in y), then it is convenient to use separable basis functions $p_i(x) p_j(y)$, e.g. products of polynomials.  While you can still form a Vandermonde-like system as above, it turns out that there are *much* more efficient algorithms in this case.  (The ideal case is a tensor product of Chebyshev points along each dimension, in which case you can interpolate via the barycentric formula in $O(n)$ cost, and via Chebyshev polynomials in $O(n log n)$.)
* If you have a tensor product grid, you can treat it as a collection of rectangles, and do [bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation) on each rectangle — this is the obvious generalization of piecewise-linear interpolation in 1d.  It is fast and second-order accurate (and there are higher-order versions like [bicubic interpolation](https://en.wikipedia.org/wiki/Bicubic_interpolation) too).
* If you have an irregular set of points, there is *still* an analogue of piecewise-linear interpolation.  One first connects the set of points into *triangles* in 2d (or tetrahedra in 3d, or more generally "simplices"); this is a tricky problem of computational geometry, but a standard and robust solution is called [Delaunay triangulation](https://en.wikipedia.org/wiki/Delaunay_triangulation).   Once this is done, which has $O(n \log n)$ cost, you can interpolate within each triangle (or simplex) using an affine function (linear + constant $a^T x + \beta$).

**Further reading**: See the further reading from lecture 4 on polynomial interpolation and the barycentric formula.  FNC book [section 2.5](https://fncbook.com/efficiency) on efficiency of solving Ax=b; much more detail on both this and the least-squares case can be found in e.g. [Trefethen & Bau's *Numerical Linear Algebra*](https://people.maths.ox.ac.uk/trefethen/text.html) and many other sources.  Links to the barycentric formula can be found above, along with fast algorithms for Chebyshev interpolation.   Tensor-product-grid interpolation and fitting products are also closely related to [Kronecker products of matrices](https://www.sciencedirect.com/science/article/pii/S0377042700003939), and there are often more efficient algorithms than simply forming the giant "Vandermonde" matrix and solving it.

**Further reading (RBFs):** A very readable overview of radial basis functions can be found in this [blog post by Shih-Chin](https://shihchinw.github.io/2018/10/data-interpolation-with-radial-basis-functions-rbfs.html).  Much deeper coverage can be found in the book [*Meshfree Approximation Methods With Matlab* (Fasshauer, 2007)](https://books.google.com/books?id=bj48DQAAQBAJ) or in this [review by Buhmann (2000)](https://www.math.ucdavis.edu/~saito/data/jim/buhmann-actanumerica.pdf).

## Lecture 8 (Feb 18)

* Quadrature: [Notes](https://math.mit.edu/~stevenj/trap-iap-2011.pdf) on error analysis of the trapezoidal rule and Clenshaw-Curtis quadrature in terms of Fourier cosine series, and a [quick review of cosine series](http://math.mit.edu/~stevenj/cosines.pdf).
* [pset 2 solutions](psets/pset2sol.ipynb)
* [pset 3](psets/pset3.ipynb): due **Thursday** (not Wed) Feb 26 at midnight

Launched into a new topic: [Numerical integration ("quadrature")](https://en.wikipedia.org/wiki/Numerical_integration).   In general, a "quadrature rule" is a scheme for estimating a definite integral ∫f(x)dx by a sum ∑ₖf(xₖ)wₖ of f(x) evaluated at N (or N+1) quadrature points xₖ with quadrature weights wₖ.   The goal is to choose these quadrature points and weights so that the estimate converges to the true integral as quickly as possible with N.  (The typical assumption is that evaluating f(x) is the dominant computational cost, so we want to minimize function evaluations.)  Quadrature is closely related to interpolation: most quadrature schemes (especially in 1d) proceed by picking points, interpolating the function between the points somehow (usually by polynomials), and then integrating the interpolant (which is easy with polynomials).

Began by analyzing the two simplest schemes with equally spaced points: piecewise-linear interpolation leads to a composite [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule) with $O(1/N^2)$ error (for continuous, piecewise-differentiable functions), while piecewise-constant interpolation leads to a [rectangle rule (Riemann sum, or Euler method)](https://en.wikipedia.org/wiki/Riemann_sum) with $O(1/N)$ error.

These error estimates are *upper* bounds, but some functions do much better!   For *periodic* functions, the trapezoidal and rectangle rules are equivalent, and for smooth (["analytic"](https://en.wikipedia.org/wiki/Analytic_function)) functions it converges *exponentially* fast with $N$.  Saw this with an example function, but our goal is to see precisely *why* this is, and then use this insight to rearrange things so that we can *always* get exponential convergence for smooth functions (even if they are not periodic).  The key to achieving this is to analyze convergence in terms of a [Fourier cosine series](https://en.wikipedia.org/wiki/Fourier_sine_and_cosine_series).


**Further reading (quadrature):**: FNC [section 5.6 on integration](https://fncbook.com/integration) and [9.6 on spectrally accurate integration](https://fncbook.com/spectral-integration/).  Lloyd N. Trefethen, "[Is Gauss quadrature better than Clenshaw-Curtis?](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.4174)," _SIAM Review_ **50** (1), 67-87 (2008).   Trefethen's [Six Myths of Polynomial Interpolation and Quadrature (2011)](https://people.maths.ox.ac.uk/trefethen/mythspaper.pdf) is a shorter and more colloquial description some of these ideas (and more!).  A related polynomial interpolation method (in some sense a generalization of quadrature by Chebyshev polynomials/points) is [Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature) (and its many variants), whose accuracy is analyzed in the Trefethen papers above, and the state of the art for computing which is [Hale and Townsend (2012)](https://ora.ox.ac.uk/objects/uuid:3b7a2384-7d36-4b23-badc-8f988197e529/files/mb03cca904f2528ea9563539c0585f8a6), implemented in [FastGaussQuadrature.jl](https://github.com/JuliaApproximation/FastGaussQuadrature.jl); a suboptimal but beautifully simple algorithm was described by [Golub & Welsch (1969)](https://www.ams.org/journals/mcom/1969-23-106/S0025-5718-69-99647-1/S0025-5718-69-99647-1.pdf).

## Lecture 9 (Feb 20)

* See notes from Lecture 8.

Continued analysis from Lecture 8 (see notes).  We related the convergence rate of trapezoidal rule to the convergence rate of the Fourier cosine series, and showed (using integration by parts) that the convergence rate of the cosine series is determined by the behavior of the odd-order derivatives at the boundaries (assuming that the function is smooth in the interior).   This reproduces the $O(1/N^2)$ convergence rate in the general case from before, and $O(1/N^4)$ convergence if the first derivatives match at the boundary, etcetera … with "superalgebraic" convergence *faster than any power law* if all the odd derivatives match (e.g. vanish) at the boundary.  (The convergence is exponential for "analytic" functions, i.e. functions with a convergent Taylor series.)

Moreover, showed how we can arrange for this fast convergence to occur all the time: for $\int_{-1}^{+1} f(x) dx$, we change variables to $f(cos \theta)$, at which point (a) the cosine series of $f(cos \theta)$ converges superalgebraically and (b) we can compute the cosine-series coefficients $a_k = \frac{2}{\pi} \int_0^\pi f(\cos\theta) \cos(k\theta) d\theta$ by a trapezoidal rule with **equally spaced θ** and it will converge superalgebraically.   Putting these together, we found that you get a quadrature rule where the points $x_n = \cos(n\pi/N)$ are Chebyshev points (the same ones we've encountered several times already), and the weights can be computed by a [discrete cosine transform (DCT)](https://en.wikipedia.org/wiki/Discrete_cosine_transform) (which can be done in $O(N \log N)$ work using an [FFT algorithm]()).  This algorithm (with some additional technical details) is known as [Clenshaw–Curtis quadrature](https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature), and is one of the two fastest converging known quadrature algorithms for generic smooth integrands (the other being [Gauss–Lobatto quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature), which turns out to have [almost the same convergence rate](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.4174)).

Finally, if we change variables back to $x$, we see that the cosine series for $f(\cos \theta)$ is precisely an expansion in [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials) $T_k(x) = \cos(k \cos^{-1} x)$, which are now revealed to be a Fourier cosine series in disguise.  This is why Chebyshev polynomials are so nicely behaved (they inherit all the nice properties of the cosine basis, but work for non-periodic functions thanks to the change of variables) and is why Chebyshev points $\cos(n\pi/N)$ were so important (they correspond to equally spaced angles for the cosine series).   In fact, Clenshaw–Curtis quadrature is exactly equivalent to evaluating the integrand at Chebyshev points, interpolating it with a polynomial, and integrating the polynomial interpolant.

**Further reading:**: See further reading for lecture 8, and for lecture 4 on Chebyshev polynomials.

## Lecture 10 (Feb 23): SNOW DAY

* pre-recorded [lecture video (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5c256f1d-33e4-4c10-b834-b27700041e44) from Spring 2025.
* [quadrature overview slides](https://docs.google.com/presentation/d/1jN49UvkUHpKLPW7gfKFQgiajNAl82x4nnAcMcXGERFQ/edit?usp=sharing): now that we've analyzed trapezoidal rule and Clenshaw–Curtis, let's zoom out to survey the bigger picture

Overview of the big picture of quadrature algorithms: Clenshaw–Curtis is not the end!

**Further reading):**: See further reading for lecture 8.  There are *many, many* more sources on numerical integration, and so many methods that even whole books leave out many topics.  For example, this review by [Bailey and Borwein (2011)](https://www.sciencedirect.com/science/article/pii/S0747717110001409) talks about high-precision (arbitrary precision) quadrature.  The book [*Methods of Numerical Integration*](https://shop.elsevier.com/books/methods-of-numerical-integration/davis/978-0-12-206360-2) is somewhat older but still a classic.
