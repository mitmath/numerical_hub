# Interdisciplinary Numerical Methods: "Hub" 18.S190/16.S090

This new MIT course (**Spring 2025**) introduces numerical methods and numerical analysis to a broad audience (assuming 18.03, 18.06, or equivalents, and some programming experience).  It is divided into two 6-unit halves:

*  **18.S190/16.S090** (first half-term “hub”): basic numerical methods, including curve fitting, root finding, numerical differentiation and integration, numerical differential equations, and floating-point arithmetic. Emphasizes the complementary concerns of accuracy and computational cost.  [Prof. Steven G. Johnson](http://math.mit.edu/~stevenj) and [Prof. Qiqi Wang](https://aeroastro.mit.edu/people/qiqi-wang/).

*   Second half-term: three options for 6-unit “spokes”

    *   18.S191/16.S091 — numerical methods for partial differential equations: finite-difference and finite-volume methods, boundary conditions, accuracy, and stability. [Prof. Qiqi Wang](https://aeroastro.mit.edu/people/qiqi-wang/).

    *   18.S097/16.S097 — large-scale linear algebra: sparse matrices, iterative methods, randomized methods. [Prof. Steven G. Johnson](http://math.mit.edu/~stevenj).

    *   18.S192/16.S098 — parallel numerical computing: multi-threading and distributed-memory, and trading off computation for parallelism — _may be taken simultaneously_ with other spokes! [Prof. Alan Edelman](https://math.mit.edu/~edelman/).

Taking both the hub and any spoke will count as an 18.3xx class for math majors, similar to 18.330, and as 16.90 for course-16 majors. Weekly homework, *no exams*, but spokes will include a final project.

This repository is for the "hub" course (currently assigned the temporary numbers 18.S190/16.S090).

## 18.S190/16.S090 Syllabus, Spring 2025

**Instructors**: [Prof. Steven G. Johnson](http://math.mit.edu/~stevenj) and [Prof. Qiqi Wang](https://aeroastro.mit.edu/people/qiqi-wang/).

**Lectures**: MWF10 in 2-142 (Feb 3 – Mar 31), slides and notes posted below.  Lecture videos posted in [Panopto Video on Canvas](https://canvas.mit.edu/courses/30272).

**Homework and grading**: 6 weekly psets, posted Fridays and due Friday *midnight*; psets are accepted up to 24 hours late with a 20% penalty; for any other accommodations, speak with [S3](https://studentlife.mit.edu/s3) and have them contact the instructors.  No exams.

* Homework assignments will require some programming — you can use either **Julia or Python** (your choice; instruction and examples will use a mix of languages).

* Submit your homework *electronically* via [Gradescope on Canvas](https://canvas.mit.edu/courses/30272) as a *PDF* containing code and results (e.g. from a Jupyter notebook) and a scan of any handwritten solutions.

* **Collaboration policy:** Talk to anyone you want to and read anything you want to, with two caveats: First, make a solid effort to solve a problem on your own before discussing it with classmates or googling. Second, no matter whom you talk to or what you read, write up the solution on your own, without having their answer in front of you (this includes ChatGPT and similar). (You can use [psetpartners.mit.edu](https://psetpartners.mit.edu/) to find problem-set partners.)

**Teaching Assistants**: [Mo Chen](https://math.mit.edu/directory/profile.html?pid=2176) and [Shania Mitra (shania at mit.edu)](https://cse.mit.edu/people/shania-mitra/)

**Office Hours**: Wednesday 4pm in 2-345 (Prof. Johnson) and Thursday 5pm via [Zoom](https://www.google.com/url?q=https://mit.zoom.us/j/94237760810&sa=D&source=calendar&usd=2&usg=AOvVaw2SiQ_dpy2RlVXTKeGITt2j) (Prof. Wang).

**Resources**: [Piazza discussion forum](https://piazza.com/mit/spring2025/16s09018s190/home), [math learning center](https://math.mit.edu/learningcenter/), [TSR^2 study/resource room](https://ome.mit.edu/programs/talented-scholars-resource-room-tsr2), [pset partners](https://psetpartners.mit.edu/).

**Textbook**: No required textbook, but suggestions for further reading will be posted after each lecture.  The book [*Fundamentals of Numerical Computation* (FNC)](https://fncbook.com/) by Driscoll and Braun is **freely available online**, has examples in Julia, Python, and Matlab, and is a valuable resource.  [*Fundamentals of Engineering Numerical Analysis* (FENA)](https://www.cambridge.org/core/books/fundamentals-of-engineering-numerical-analysis/D6B6B75172AD7A5A555DC506FDDA9B99) by Moin is another useful resource ([readable online](https://www-cambridge-org.libproxy.mit.edu/core/books/fundamentals-of-engineering-numerical-analysis/D6B6B75172AD7A5A555DC506FDDA9B99) with MIT certificates).

This document is a *brief* summary of what was covered in each
lecture, along with links and suggestions for further reading.  It is
*not* a good substitute for attending lecture, but may provide a
useful study guide.

## Lecture 1 (Feb 3)

* Overview and syllabus: [slides](https://docs.google.com/presentation/d/1zhe_c1rgfLuDNH3h3VIPu5W3Ic821wighMAjepF_F2g/edit?usp=sharing) and this web page
* Finite-difference approximations: [Julia notebook and demo](notes/finite-differences.ipynb)

Brief overview of the huge field of numerical methods, and outline of the small portion that this course will cover. Key new concerns in numerical analysis, are (i) performance (traditionally, arithmetic counts, but now memory access often dominates) and (ii) accuracy (both floating-point roundoff errors and also convergence of intrinsic approximations in the algorithms).  In contrast, the more pure, abstract mathematics of continuity is called "analysis", and is mainly concerned with (ii) but not (i): they are happy to prove that limits converge, but don't care too much how *quickly* they converge.  Whereas traditional discrete computer science is concerned with mainly with (i) but not (ii): they care about performance and resource usage, but traditional algorithms like sorting are either right or wrong, never approximate.

As a starting example, considered the the convergence of **finite-difference approximations** to derivatives df/dx of given functions f(x), which appear in many areas of numerical analysis (such as solving differential equations) and are also closely tied to *polynomial approximation and interpolation*.   By examining the errors in the finite-difference approximation, we immediately see *two* competing sources of error: *truncation* error from the non-infinitesimal Δx, and *roundoff* error from the finite precision of the arithmetic.  Understanding these two errors will be the gateway to many other subjects in numerical methods.

**Further reading**: [FNC book: Finite differences](https://fncbook.com/finitediffs), [FENA book: chapter 2](https://www-cambridge-org.libproxy.mit.edu/core/books/fundamentals-of-engineering-numerical-analysis/numerical-differentiation-finite-differences/96E5E7ED237DF7FD71F8B0F7DCA2EF7A).  There is a lot of information online on [finite difference approximations](https://en.wikipedia.org/wiki/Finite_difference),  [these 18.303 notes](https://github.com/mitmath/18303/blob/fall16/difference-approx.pdf), or [Section 5.7 of *Numerical Recipes*](http://www.it.uom.gr/teaching/linearalgebra/NumericalRecipiesInC/c5-7.pdf).   The Julia [FiniteDifferences.jl package](https://github.com/JuliaDiff/FiniteDifferences.jl) provides lots of algorithms to compute finite-difference approximations; a particularly robust and powerful way to obtain high accuracy is to employ [Richardson extrapolation](https://github.com/JuliaDiff/FiniteDifferences.jl#richardson-extrapolation) to smaller and smaller δx.  If you make δx too small, the finite precision (#digits) of [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic) leads to [catastrophic cancellation errors](https://en.wikipedia.org/wiki/Catastrophic_cancellation).

## Lecture 2 (Feb 5)

* [Floating-point introduction](notes/Floating-Point-Intro.ipynb)

One of the most basic sources of computational error is that **computer arithmetic is generally inexact**, leading to [roundoff errors](https://en.wikipedia.org/wiki/Round-off_error).  The reason for this is simple: computers can only work with numbers having a **finite number of digits**, so they **cannot even store** arbitrary real numbers.  Only a _finite subset_ of the real numbers can be represented using a particular number of "bits", and the question becomes _which subset_ to store, how arithmetic on this subset is defined, and how to analyze the errors compared to theoretical exact arithmetic on real numbers.

In **floating-point** arithmetic, we store both an integer coefficient and an exponent in some base: essentially, scientific notation. This allows large dynamic range and fixed _relative_ accuracy: if fl(x) is the closest floating-point number to any real x, then |fl(x)-x| < ε|x| where ε is the _machine precision_. This makes error analysis much easier and makes algorithms mostly insensitive to overall scaling or units, but has the disadvantage that it requires specialized floating-point hardware to be fast. Nowadays, all general-purpose computers, and even many little computers like your cell phones, have floating-point units.

Went through some simple definitions and examples in Julia (see notebook above), illustrating the basic ideas and a few interesting tidbits.  In particular, we looked at **error accumulation** during long calculations (e.g. summation), as well as examples of [catastrophic cancellation](https://en.wikipedia.org/wiki/Loss_of_significance) and how it can sometimes be avoided by rearranging a calculation.

**Further reading:** [FNC book: Floating-poing numbers](https://fncbook.com/floating-point).  [Trefethen & Bau's *Numerical Linear Algebra*](https://people.maths.ox.ac.uk/trefethen/text.html), lecture 13. [What Every Computer Scientist Should Know About Floating Point Arithmetic](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.6768) (David Goldberg, ACM 1991). William Kahan, [How Java's floating-point hurts everyone everywhere](http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf) (2004): contains a nice discussion of floating-point myths and misconceptions.   A brief but useful summary can be found in [this Julia-focused floating-point overview](https://discourse.julialang.org/t/psa-floating-point-arithmetic/8678) by Prof. John Gibson. Because many programmers never learn how floating-point arithmetic actually works, there are [many common myths](https://github.com/mitmath/18335/blob/spring21/notes/fp-myths.pdf) about its behavior.   (An infamous example is `0.1 + 0.2` giving `0.30000000000000004`, which people are puzzled by so frequently it has led to a web site [https://0.30000000000000004.com/](https://0.30000000000000004.com/)!)

## Lecture 3 (Feb 7)

* Interpolation [OneNote Notebook](https://mitprod-my.sharepoint.com/:o:/g/personal/qiqi_mit_edu/EtIbMiRpjBpFsMzCtCrtV0MBy0J_u2YB9ltGUK90gXHhuQ?e=qwoIrA) [Code](https://colab.research.google.com/drive/1khUewCdh5Io97dry6O5pZNaP7_4w8QFC?usp=sharing) [Complete Notes](https://colab.research.google.com/drive/1NyU48yqYY91h2a6sZZvr1WcFUxCiDZXS?usp=sharing)
* [pset 1](psets/pset1.ipynb): due Feb 14

Discussed the important problem of **interpolating** a function $f(x)$ from a set of discrete data points, which shows up in a vast number of real problems and connects to many other areas of numerical methods (e.g. differentiation and integration).  To begin with, considered the simplest algorithm of [piecewise linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) in one dimension, with points $x$ at spacing $\Delta x$, and showed that (for a twice-differentiable function) the error (the difference between the interpolant and the true function) converges as $O(\Delta x^2)$ (second-order convergence).

**Further reading:** [FNC chapter 5](https://fncbook.com/overview-4) and FENA chapter 1.  Piecewise linear interpolation is implemented in Python by [`numpy.interp`](https://numpy.org/doc/stable/reference/generated/numpy.interp.html#numpy.interp), and several other interpolation schemes by [`scipy.interpolate`](https://docs.scipy.org/doc/scipy-1.15.1/tutorial/interpolate.html).  Interpolation packages in Julia include [Interpolations.jl](https://github.com/JuliaMath/Interpolations.jl), [Dierckx.jl (splines)](https://github.com/JuliaMath/Dierckx.jl),  [BasicInterpolators.jl](https://github.com/markmbaum/BasicInterpolators.jl), and [FastChebInterp.jl (high-order polynomials)](https://github.com/JuliaMath/FastChebInterp.jl).

## Optional Julia Tutorial (Feb 7 @ 4pm in 2-190)

A basic overview of the Julia programming environment for numerical computations.   This tutorial will cover what Julia is and the basics of interaction, scalar/vector/matrix arithmetic, and plotting — just as a "fancy calculator" for now (without the "real programming" features).

* [Tutorial materials](https://github.com/mitmath/julia-mit) (and links to other resources)

If possible, try to install Julia on your laptop beforehand using the instructions at the above link.  Failing that, you can run Julia in the cloud (see instructions above).

This won't be recorded, but you can find a [video of a similar tutorial by Prof. Johnson last year (MIT only)](https://mit.zoom.us/rec/share/oirQFHELtxJkopybssFzml7YrudRyvIlmXjmgq4YemqmjT0P7wMGCd9ilC7SMZ_o.iBZ-UO6_ww9WjwF0?startTime=1705960822000), as well as many other tutorial videos at [julialang.org/learning](https://julialang.org/learning/).

## Lecture 4 (Feb 10)
* Notes: [OneNote Notebook](https://mitprod-my.sharepoint.com/:o:/g/personal/qiqi_mit_edu/EtIbMiRpjBpFsMzCtCrtV0MBy0J_u2YB9ltGUK90gXHhuQ?e=qwoIrA)

One approach to generalizing piecewise-linear interpolation is to interpolate $n$ points with a polynomial $p(x)$ of degree of degree $n-1$.  This is an important technique for many applications, and the general topic of approximating functions by polynomials has vast importance in numerical analysis, but requires care if $n$ becomes much larger than 4 (cubic) or so, even for smooth functions (no noise).

A general conceptual approach is to set up a system of linear equations for the polynomial coefficients $c_i$, satisfying $p(x_k) = y_k$ for each data point $(x_k, y_k)$.  This can be expressed in matrix form $Ac = y$, where the $n \times n$ matrix $A$ is known as a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix) if you are using the usual monomial basis $p(x) = c_0 + c_1 x + c_2 x^2 + \cdots$.   One quickly runs into two difficulties, however:

1. The matrix $A$ is **nearly singular** for large $n$ in the monomial basis, so floating-point roundoff errors are exacerbated.  (We will say that it has a large [condition number](https://en.wikipedia.org/wiki/Condition_number) or is "ill-conditioned", and will define this more precisely next time.)   **Solution:** It turns out that monomials are just a poorly behaved basis for high-degree polynomials, and it is much better to use [orthogonal polynomials](https://en.wikipedia.org/wiki/Orthogonal_polynomials), most commonly [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials), as a basis — with these, people regularly go to degrees of thousands or even millions.

2. Polynomial interpolation from **equally spaced points** (in *any* basis!) can **diverge exponentially** from the underlying "true" smooth function **in between the points** (even in exact arithmetic, with no roundoff errors!).  This is called a [Runge phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon).   **Solution 1**: Use carefully chosen non-equispaced points.  A good choice that leads to *exponentially good* polynomial approximations (for smooth functions) is the [Chebyshev nodes](https://en.wikipedia.org/wiki/Chebyshev_nodes), which are clustered near the endpoints.  **Solution 2**: use a *lower* degree ($<< n$) polynomial and perform an *approximate fit* to the given points, rather than requiring the polynomial to go through them exactly.  (More on this soon.)

If you address both of these problems, high-degree polynomial approximation can be a fantastic tool for describing *smooth* functions.  If you have *noisy* data, however, you should typically use much lower-degree polynomials to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting) (trying to match the noise rather than the underlying "true" function).

**Further reading**: [FNC book, chapter 9](https://fncbook.com/polynomial).  Beware that the FENA book starts with the ["Lagrange formula"](https://en.wikipedia.org/wiki/Lagrange_polynomial) for the interpolating polynomial, but this formula is very badly behaved ("numerically unstable") for high degrees and should not be used; it is superseded by the "barycentric" Lagrange formula (see FNC book; [reviewed in much more detail by Berrut and Trefethen, 2004](https://people.maths.ox.ac.uk/trefethen/barycentric.pdf)).  The subject of polynomial interpolation is the entry point into [approximation theory](https://en.wikipedia.org/wiki/Approximation_theory); if you are interested, the [book by Trefethen](https://people.maths.ox.ac.uk/trefethen/ATAP/) and accompanying [video lectures](https://people.maths.ox.ac.uk/trefethen/atapvideos.html) are a great place to get more depth.    The [`numpy.polynomials`](https://numpy.org/doc/2.2/reference/routines.polynomials.html) module contains a variety of functions for polynomial interpolation, including with Chebyshev polynomials.  A package for multi-dimensional Chebyshev polynomial interpolation in Julia is [FastChebInterp](https://github.com/JuliaMath/FastChebInterp.jl).  A pioneering package that shows off the power of Chebyshev polynomial interpolation is [`chebfun` in Matlab](https://www.chebfun.org/), along with [related packages in other languages](https://www.chebfun.org/about/projects.html).  (This approach is taken to supercomputing extremes by the [Dedalus package](https://dedalus-project.org/) to solve partial differential equations using exponentially converging polynomial approximations.)   It turns out that if you are interpolating using Chebyshev polynomials, and you choose your points $x_k$ to be Chebyshev points (either extrema or roots of the degree $n$ Chebyshev polynomial), then you don't need to form any Vandermonde-like matrix explicitly — you can get the coefficients *much* more quickly, in $O(n \log n)$ operations, using [fast Fourier transforms (FFTs)](https://en.wikipedia.org/wiki/Fast_Fourier_transform); given these, a polynomial in the Chebyshev basis can then be evaluated at any $x$ with $O(n)$ operations using the [Clenshaw algorithm](https://en.wikipedia.org/wiki/Clenshaw_algorithm).  If you don't need the explicit coefficients, the barycentric Lagrange formula (for an arbitrary set of points) has $O(n^2)$ setup cost to obtain a set of weights from the $x_k$ — still cheaper than the $O(n^3)$ cost of solving the Vandermonde system — after which the polynomial can be evaluated cheaply ($`O(n)`$) at any point $x$ similar to the other schemes.

## Lecture 5 (Feb 12)

* [handwritten notes](https://www.dropbox.com/scl/fi/m691ghxq38a6lnoayrfxz/Condition-Numbers.pdf?rlkey=3x45kla27gcve0l2qv0d76sxi&st=i7kdwkps&dl=0)

The goal of this lecture is to precisely define the notion of a [condition number](https://en.wikipedia.org/wiki/Condition_number), which quantifies the *sensitivity* of a function f(x) to *small perturbations* in the input.  A function that has a "large" condition number is called "ill-conditioned", and *any* algorithm to compute it may suffer inaccurate results from any sort of error (whether it be roundoff errors, input noise, or other approximations) — it doesn't mean you can't use that function, but it usually means you need to be careful (about both implementation and about interpretation of the results).

For a given function $f(x)$ (with inputs and outputs in any normed vector space), the two important quantities are:
* **absolute** condition number $\kappa_a(f, x) = \lim_{\epsilon \to 0^+} \sup_{\Vert \delta x \Vert = \epsilon} \frac{\Vert \overbrace{f(x + \delta x) - f(x)}^{\delta f} \Vert}{\Vert \delta x \Vert}$.  It is the *worst case* $\Vert \delta f \Vert / \Vert \delta x \Vert$ for any arbitrarily small input perturbation $\delta x$.   Unfortunately, if the inputs and outputs of $f$ have different scales ("units"), then $\kappa_a$ may be hard to interpret (it is "dimensionful").
* **relative** condition number $\kappa_r(f, x) = \lim_{\epsilon \to 0^+} \sup_{\Vert \delta x \Vert = \epsilon} \frac{\Vert \delta f \Vert / \Vert f(x) \Vert}{\Vert \delta x \Vert / \Vert x \Vert} = \kappa_a(f, x) \frac{\Vert  x \Vert}{\Vert f(x) \Vert}$.  This is a dimensionless quantity: it is the maximum ratio of the *relative change in f* to the *relative change in x*.   Most of the time, $\kappa_r$ is the quantity of interest.

All of these quantities involve the central concept of a [norm ‖⋯‖](https://en.wikipedia.org/wiki/Normed_vector_space), which is a way of measuring the "length" of a vector.   The most familar norm, and usually the default or implicit choice for column vectors $x \in \mathbb{R}^n$, is the [Euclidean or "L₂" norm](https://mathworld.wolfram.com/L2-Norm.html) $\sqrt{x^T x}$, but other popular norms include the [L₁ norm](https://mathworld.wolfram.com/L1-Norm.html) (the ["taxicab" norm](https://en.wikipedia.org/wiki/Taxicab_geometry)) and the [maximum norm](https://en.wikipedia.org/wiki/Chebyshev_distance) ($L_\infty$).  It turns out that all norms differ by [at most an n-dependent constant factor](https://github.com/mitmath/18335/blob/spring21/notes/norm-equivalence.pdf), so the choice of norms is mostly one of convenience if you don't care about constant factors, e.g. if you are comparing $O(\varepsilon)$ to $O(\varepsilon^2)$, but it can make a big difference in optimization/fitting (e.g. different forms of regression or regularization).

For example, looked at the condition number of summation $f(x) = \sum_i x_i$.  In the $L_1$ norm, the absolute condition number is $\kappa_a = 1$, and the relative condition number is $\kappa_r = \sum_i |x_i| / |\sum_j x_j |$, which gives $\kappa_r = 1$ when all the summands $x_i$ have the same sign, but blows up $\to \infty$ as $\sum_j x_j \to 0$ (for $x \ne 0$) — this is an "ill-conditioned" sum where you can expect a large *relative* error due to [catastrophic cancellation](https://en.wikipedia.org/wiki/Catastrophic_cancellation), similar to the examples in lectures 1 and 2.

If the function is differentiable, then the condition number simplifies even further:
* If $x$ and $f(x)$ are scalars, then $\kappa_a = |f'(a)|$ is simply the magnitude of the derivative.
* If $x \in \mathbb{R}^n$ and $f(x) \in \mathbb{R}^m$ are vectors, then $\kappa_a = \Vert J \Vert$ is the ["operator" or "induced" norm](https://mathworld.wolfram.com/OperatorNorm.html) of the [Jacobian matrix J](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) ($J_{i,j} = \partial f_i /\partial x_j$), where the induced norm $\Vert A \Vert = \sup_{x \ne 0} \frac{\Vert A x \Vert}{\Vert x \Vert}$ measures "how big" a matrix $A$ is by how much it can *stretch* a vector.  (We won't worry too much yet about how to *compute* this induced norm, but if you know the [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) it is the largest singular value of $A$.)

This leads us to our next important concept, the (relative) condition number $\kappa(A)$ *of a matrix*, and in particular we considered square invertible matrices $A$.   This is defined by first considering the function $f(x) = Ax$.  Its relative condition number, from the above definitions, is $\kappa_r(f, x) = \Vert A \Vert \frac{\Vert x \Vert}{\Vert Ax \Vert}$, which depends upon $x$.  However, if we take the *upper bound* over *all x*, i.e. the *worst x*, then we get $\kappa_r(f, x) \le \Vert A \Vert \cdot \Vert A^{-1} \Vert = \kappa(A)$, the product of the norms of $A$ and its inverse.  Note that $\kappa(A) = \kappa(A^{-1})$: this also tells us the worst case sensitivity for *solving Ax=b* (multiplying by the inverse).  (If you know the SVD, then $\kappa$ is the ratio of the largest and smallest singular values of $A$.)   The condition number is a *dimensionless* (scale-invariant) measure of **how close to singular** the matrix $A$ is.  For example, if two columns of $A$ are nearly parallel, then $\kappa(A)$ will be very large: we say that the matrix is "ill-conditioned" and you should be very careful of inaccuracies when working with it.

For example, we can now explain why the monomial basis was so bad: it is easy to see that the [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix) becomes nearly singular for large $n$, since when you take large powers the columns become nearly parallel.  Conversely, it turns out that a "rotation matrix", or more generally any [orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix), has condition number equal to **1** (in the $L_2$ norm), and correspondingly using orthogonal polynomials is a much better-behaved basis.

**Further reading:** FNC book: [problems and conditioning](https://fncbook.com/conditioning) and
[conditioning linear systems](https://fncbook.com/condition-number#index-tqii4dtlc9).  [18.06 lecture notes on conditioning of linear systems](https://nbviewer.org/github/stevengj/1806/blob/fall18/lectures/Conditioning.ipynb).   Advanced books like [*Numerical Linear Algebra* by Trefethen and Bau](https://www.stat.uchicago.edu/~lekheng/courses/309/books/Trefethen-Bau.pdf) (lecture 12) treat this subject in much more depth.  See also Trefethen, lecture 3, for more in-depth coverage of norms.  A fancy vocabulary word for a vector space with a norm (plus some technical criteria) is a [Banach space](https://en.wikipedia.org/wiki/Banach_space).

## Lecture 6 ([Feb 14 💕](https://en.wikipedia.org/wiki/Valentine%27s_Day))

* [pset 1 solutions](https://nbviewer.org/github/mitmath/numerical_hub/blob/main/psets/pset1sol.ipynb)
* [pset 2](psets/pset2.ipynb): due Feb 21
* Notes: [OneNote Notebook](https://mitprod-my.sharepoint.com/:o:/g/personal/qiqi_mit_edu/EtIbMiRpjBpFsMzCtCrtV0MBy0J_u2YB9ltGUK90gXHhuQ?e=qwoIrA)

Introduced the topic of least-square fitting of data to curves.  As long as the fitting function is linear in the unknown coefficients c (e.g. a polynomial, or some other linear combination of basis functions), showed that minimizing the sum of the squares of the errors corresponds to minimizing the norm of the residual, i.e. the "loss function" L(c) = ‖Ac - y‖², where $A$ is a "tall" matrix whose *rows* correspond to the data points and whose *columns* correspond to the basis functions.  (This is an *overdetermined* linear system because there are more equations than unknowns: we have too few parameters to make the curve go through all the data points in general, unlike interpolation.)

It is a straightforward calculus exercise to show that ∇L = 2Aᵀ(Ac - y), which means that optimal coefficients c can be found by setting the gradient to zero, and ∇L=0 implies the "normal equations" AᵀAc = Aᵀy.   In principle, these can be solved directly, the normal equations square the condition number of A (κ(AᵀA)=κ(A)²) so they are normally solved in a different way, typically by [QR factorization](https://en.wikipedia.org/wiki/QR_decomposition) of A (or sometimes using the [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) of A); there are typically library functions that do this for you e.g. `c = numpy.linalg.lstsq(A, y)` in Python or `c = A \ y` in Julia and Matlab.

More generally, minimizing L(c) is an example of an [optimization problem](https://en.wikipedia.org/wiki/Mathematical_optimization).  One simple way to attack such problems is by [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent): simply go downhill, in steps Δc = -s∇L where s is the "learning rate" or "step size" parameter (that has to be chosen carefully: too small and it will converge very slowly, but too large and it won't converge at all).  It turns out that an ill-conditioned A also leads to gradient descent that converges slowly (because the local downhill direction -∇L doesn't point to the minimum, necessitating small steps).   Nowadays, naive gradient descent (especially with a fixed learning rate) is a rather primitive technique that has been mostly superseded by better "accelerated" methods, but computing the gradient ∇L and using it to identify the downhill direction is still a key conceptual starting point for many algorithms.   Of course, for least-square fitting where L(c) is linear in c, we can directly solve for c as shown above, but iterative optimization methods are crucial to solve more general problems where the unknowns enter in a nonlinear way (and/or there are constraints).

In both cases, compared the effect of the monomial basis vs. the Chebyshev-polynomial basis.  Because the former leads to an ill-conditioned $A$ at high degrees, the accuracy of the fit quickly saturates due to roundoff errors, especially if you solve AᵀAc = Aᵀy (which squares the condition number); it also leads to slow convergence for gradient descent.  The Chebyshev basis, in contrast, leads to a well-conditioned $A$ (even for equally spaced points, as long as the number of points is much larger than the degree), so it has neither of these problems.

**Further reading:** [FNC book chapter 3](https://fncbook.com/overview-2).  Strang's [*Introduction to Linear Algebra*](https://math.mit.edu/~gs/linearalgebra/ila6/indexila6.html) section 4.3 and [18.06 videow lecture 16](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-16-projection-matrices-and-least-squares/).   There are many, many books and other materials on [linear least-squares fitting](https://en.wikipedia.org/wiki/Linear_least_squares), from many different perspectives (e.g. numerical linear algebra, statistics, machine learning…) that you can find online.  The [FastChebInterp.jl package](https://github.com/JuliaMath/FastChebInterp.jl) in Julia does least-square fitting ("regression") with Chebyshev polynomials for you in an optimized way, including multi-dimensional fitting.

## Lecture 7 (TUESDAY, Feb 18)

First, discussed the computational cost of interpolation and fitting.
* Solving the least-squares problem $\min_x \Vert Ax - b\Vert$ for an $m \times n$ matrix $A$ (m points and n basis functions) has $O(mn^2)$ cost, whether you use the normal equations $A^T A x = A^T b$ (which squares the condition number) or a more accurate method like QR factorization.
* Interpolation can be thought of as the special case $m=n$: solving an $n \times n$ linear system $Ax =b$ is $O(n^3)$ (usually done by [LU factorization](https://en.wikipedia.org/wiki/LU_decomposition), which is the matrix-factor form of Gaussian elimination).   However, for specific cases there are more efficient algorithms: for polynomial interpolation from arbitrary points, the barycentric Lagrange formula (mentioned above) has $O(n^2)$ cost, and using the Chebyshev-polynomial basis from Chebyshev points has $O(n \log n)$ cost (via FFTs).

The computational scaling becomes even more important when you go to higher dimensions: [multivariate interpolation](https://en.wikipedia.org/wiki/Multivariate_interpolation) and fitting.  We discussed a few cases cases, using two dimensions (2d) for simplicity:
* If you have a general basis $b_k(x,y)$ of 2d basis functions, you can form a Vandermonde-like matrix $A$ as above (whose columns are hte basis functions and whose rows are the grid points), and solve it as before.  The matrices get much larger in higher dimensions, though!
* If you can choose a Cartesian "grid" of points, also known as a "tensor product grid" (a grid in x by a grid in y), then it is convenient to use separable basis functions $p_i(x) p_j(y)$, e.g. products of polynomials.  While you can still form a Vandermonde-like system as above, it turns out that there are *much* more efficient algorithms in this case.  (The ideal case is a tensor product of Chebyshev points along each dimension, in which case you can interpolate products of Chebyshev polynomials in $O(n \log n)$ cost.)
* If you have a tensor product grid, you can treat it as a collection of rectangles, and do [bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation) on each rectangle — this is the obvious generalization of piecewise-linear interpolation in 1d.  It is fast and second-order accurate (and there are higher-order versions like [bicubic interpolation](https://en.wikipedia.org/wiki/Bicubic_interpolation) too).
* If you have an irregular set of points, there is *still* an analogue of piecewise-linear interpolation.  One first connects the set of points into *triangles* in 2d (or tetrahedra in 3d, or more generally "simplices"); this is a tricky problem of computational geometry, but a standard and robust solution is called [Delaunay triangulation](https://en.wikipedia.org/wiki/Delaunay_triangulation).   Once this is done, you can interpolate within each triangle (or simplex) using an affine function (linear + constant $a^T x + \beta$).

**Further reading**: FNC book [section 2.5](https://fncbook.com/efficiency) on efficiency of solving Ax=b; much more detail on both this and the least-squares case can be found in e.g. [Trefethen & Bau's *Numerical Linear Algebra*](https://people.maths.ox.ac.uk/trefethen/text.html) and many other sources.  Links to the barycentric formula can be found above, along with fast algorithms for Chebyshev interpolation.   Tensor-product-grid interpolation and fitting products are also closely related to [Kronecker products of matrices](https://www.sciencedirect.com/science/article/pii/S0377042700003939), and there are often more efficient algorithms than simply forming the giant "Vandermonde" matrix and solving it.

## Lecture 8 (Feb 19)

* Radial basis function (RBF) interpolation: [slides](https://docs.google.com/presentation/d/1NnMCzkM3LhFooB7VIDBIr4ZsvOhhRaABmbb0-9vvCd4/edit?usp=sharing)
* Quadrature: [Notes](https://math.mit.edu/~stevenj/trap-iap-2011.pdf) on error analysis of the trapezoidal rule and Clenshaw-Curtis quadrature in terms of Fourier cosine series, and a [quick review of cosine series](http://math.mit.edu/~stevenj/cosines.pdf).

To begin with, briefly touched an another popular method for multidimensional interpolation: [radial basis functions](https://en.wikipedia.org/wiki/Radial_basis_function). This can be understood in the same computational framework as polynomial interpolation: we form a "Vandermonde" matrix equation Ac=b, where the rows are data points and the columns are basis functions, to solve for the coefficients c.  In this case, the basis functions are $\Phi(\Vert x - x_k\Vert)$: a function $\Phi$ centered at every data point $x_k$, depending only on the radius from that point.  (Often decaying with radius, but not always.)  Usually there is a "hyperparameter" of a lengthscale in $\Phi$ (e.g. a localization lengthscale) that one has to set (e.g. heuristically, or by checking the fit against separate ["validation data"](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)).  Often, one *augments* the RBFs with *additional* basis functions, such as low-degree polynomials, leading to an [underdetermined system](https://en.wikipedia.org/wiki/Underdetermined_system) that one solves by imposing additional criteria (e.g. orthogonality between RBF coefficients and the polynomial terms, or perhaps minimizing the magnitude of the coefficients in some norm).

Launched into a new topic: [Numerical integration ("quadrature")](https://en.wikipedia.org/wiki/Numerical_integration).   In general, a "quadrature rule" is a scheme for estimating a definite integral ∫f(x)dx by a sum ∑ₖf(xₖ)wₖ of f(x) evaluated at N (or N+1) quadrature points xₖ with quadrature weights wₖ.   The goal is to choose these quadrature points and weights so that the estimate converges to the true integral as quickly as possible with N.  (The typical assumption is that evaluating f(x) is the dominant computational cost, so we want to minimize function evaluations.)  Quadrature is closely related to interpolation: most quadrature schemes (especially in 1d) proceed by picking points, interpolating the function between the points somehow (usually by polynomials), and then integrating the interpolant (which is easy with polynomials).

Began by analyzing the two simplest schemes with equally spaced points: piecewise-linear interpolation leads to a composite [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule) with $O(1/N^2)$ error (for continuous, piecewise-differentiable functions), while piecewise-constant interpolation leads to a [rectangle rule (Riemann sum, or Euler method)](https://en.wikipedia.org/wiki/Riemann_sum) with $O(1/N)$ error.

These error estimates are *upper* bounds, but some functions do much better!   For *periodic* functions, the trapezoidal and rectangle rules are equivalent, and for smooth (["analytic"](https://en.wikipedia.org/wiki/Analytic_function)) functions it converges *exponentially* fast with $N$.  Saw this with an example function, but our goal is to see precisely *why* this is, and then use this insight to rearrange things so that we can *always* get exponential convergence for smooth functions (even if they are not periodic).  The key to achieving this is to analyze convergence in terms of a [Fourier cosine series](https://en.wikipedia.org/wiki/Fourier_sine_and_cosine_series).

**Further reading (RBFs):** A very readable overview of radial basis functions can be found in this [blog post by Shih-Chin](https://shihchinw.github.io/2018/10/data-interpolation-with-radial-basis-functions-rbfs.html).  Much deeper coverage can be found in the book [*Meshfree Approximation Methods With Matlab* (Fasshauer, 2007)](https://books.google.com/books?id=bj48DQAAQBAJ) or in this [review by Buhmann (2000)](https://www.math.ucdavis.edu/~saito/data/jim/buhmann-actanumerica.pdf).

**Further reading (quadrature):**: FNC [section 5.6](https://fncbook.com/integration).  Lloyd N. Trefethen, "[Is Gauss quadrature better than Clenshaw-Curtis?](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.4174)," _SIAM Review_ **50** (1), 67-87 (2008).   Trefethen's [Six Myths of Polynomial Interpolation and Quadrature (2011)](https://people.maths.ox.ac.uk/trefethen/mythspaper.pdf) is a shorter and more colloquial description some of these ideas (and more!).  A related polynomial interpolation method (in some sense a generalization of quadrature by Chebyshev polynomials/points) is [Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature) (and its many variants), whose accuracy is analyzed in the Trefethen papers above, and the state of the art for computing which is [Hale and Townsend (2012)](https://ora.ox.ac.uk/objects/uuid:3b7a2384-7d36-4b23-badc-8f988197e529/files/mb03cca904f2528ea9563539c0585f8a6); a suboptimal but beautifully simple algorithm was described by [Golub & Welsch (1969)](https://www.ams.org/journals/mcom/1969-23-106/S0025-5718-69-99647-1/S0025-5718-69-99647-1.pdf).

## Lecture 9 (Feb 21)

* [pset 2 solutions](psets/pset2sol.ipynb)
* [pset 3](psets/pset3.ipynb): due Friday, Feb 28.

Continued analysis from Lecture 8 (see notes).  We related the convergence rate of trapezoidal rule to the convergence rate of the Fourier cosine series, and showed (using integration by parts) that the convergence rate of the cosine series is determined by the behavior of the odd-order derivatives at the boundaries (assuming that the function is smooth in the interior).   This reproduces the $O(1/N^2)$ convergence rate in the general case from before, and $O(1/N^4)$ convergence if the first derivatives match at the boundary, etcetera … with "superalgebraic" convergence *faster than any power law* if all the odd derivatives match (e.g. vanish) at the boundary.  (The convergence is exponential for "analytic" functions, i.e. functions with a convergent Taylor series.)

Moreover, showed how we can arrange for this fast convergence to occur all the time: for $\int_{-1}^{+1} f(x) dx$, we change variables to $f(cos \theta)$, at which point (a) the cosine series of $f(cos \theta)$ converges superalgebraically and (b) we can compute the cosine-series coefficients $a_k = \frac{2}{\pi} \int_0^\pi f(\cos\theta) \cos(k\theta) d\theta$ by a trapezoidal rule with **equally spaced θ** and it will converge superalgebraically.   Putting these together, we found that you get a quadrature rule where the points $x_n = \cos(n\pi/N)$ are Chebyshev points (the same ones we've encountered several times already), and the weights can be computed by a [discrete cosine transform (DCT)](https://en.wikipedia.org/wiki/Discrete_cosine_transform) (which can be done in $O(N \log N)$ work using an [FFT algorithm]()).  This algorithm (with some additional technical details) is known as [Clenshaw–Curtis quadrature](https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature), and is one of the two fastest converging known quadrature algorithms for generic smooth integrands (the other being [Gauss–Lobatto quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature), which turns out to have [almost the same convergence rate](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.4174)).

Finally, if we change variables back to $x$, we see that the cosine series for $f(\cos \theta)$ is precisely an expansion in [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials) $T_k(x) = \cos(k \cos^{-1} x)$, which are now revealed to be a Fourier cosine series in disguise.  This is why Chebyshev polynomials are so nicely behaved (they inherit all the nice properties of the cosine basis, but work for non-periodic functions thanks to the change of variables) and is why Chebyshev points $\cos(n\pi/N)$ were so important (they correspond to equally spaced angles for the cosine series).   In fact, Clenshaw–Curtis quadrature is exactly equivalent to evaluating the integrand at Chebyshev points, interpolating it with a polynomial, and integrating the polynomial interpolant.

**Further reading:**: See further reading for lecture 8, and for lecture 4 on Chebyshev polynomials.

## Lecture 10 (Feb 22)

* [quadrature overview slides](https://docs.google.com/presentation/d/1jN49UvkUHpKLPW7gfKFQgiajNAl82x4nnAcMcXGERFQ/edit?usp=sharing): now that we've analyzed trapezoidal rule and Clenshaw–Curtis, let's zoom out to survey the bigger picture

Overview of the big picture of quadrature algorithms: Clenshaw–Curtis is not the end!

## Lecture 11 (Feb 26)

* [handwritten notes](https://www.dropbox.com/scl/fi/igw7n111agkb4dwlzqnb9/Richardson-Extrapolation.pdf?rlkey=tcp7wkqfxdaf16xim3qe9xd9d&st=pz3ze3wf&dl=0)

Discussed [Richardson extrapolation](https://en.wikipedia.org/wiki/Richardson_extrapolation), which is a powerful general technique to compute limits $\lim_{h \to 0^+} y(h)$ by evaluating $y(h_i)$ at a decreasing sequence of $h_i > 0$ values (usually a [geometric progression](https://en.wikipedia.org/wiki/Geometric_progression)) and extrapolating polynomial interpolations.   The distinguishing features of Richardson's method are:

* It computes *many* extrapolations, formed by degree-$q$ polynomial interpolations of *all consecutive subsequences* of $q+1$ $h_i, y(h_i)$ points for all possible degrees $q$.
* All of these extrapolations are computed efficiently at the same time (in $O(n^2)$ cost for $n$ points) by a [linear recurrence relation](https://en.wikipedia.org/wiki/Recurrence_relation) where each degree-$q$ extrapolation is computed from two degree-$(q-1)$ extrapolations (forming a table called a "Neville tableau" or "Neville–Aitken tableau": it is a version of [Neville's algorithm](https://en.wikipedia.org/wiki/Neville%27s_algorithm) for polynomial interpolation).
* Each degree $q > 0$ extrapolant comes with an error estimate, given by its difference from the degree-$(q-1)$ estimate ending at the same $h_i$ point.   This allows Richardson's method to be **robust and adaptive**: by using the extrapolant with the *smallest error estimate* for the final $y(0^+)$ result, it automatically selects a good subsequence of $h_i$ values (neither so large that polynomial fitting doesn't work, nor so small that e.g. roundoff/cancellation errors dominate).

Famous applications of Richardson extrapolation include [Romberg integration](https://en.wikipedia.org/wiki/Romberg%27s_method) (extrapolating low-order quadrature formulas), the [Bulirsch–Stoer ODE method](https://en.wikipedia.org/wiki/Bulirsch%E2%80%93Stoer_algorithm) (extrapolating ODE solutions to zero stepsize), and an [algorithm by Ridders (1982)](https://www.sciencedirect.com/science/article/pii/S0141119582800570) for extrapolating finite-difference derivatives (also reviewed in [*Numerical Recipes* sec. 5.7](http://www.foo.be/docs-free/Numerical_Recipe_In_C/c5-7.pdf)).

**Further reading:** Unfortunately, almost all descriptions of Richardson extrapolation combine it with a particular application, e.g. many textbooks only describe it in the context of ODEs, or integration, or differentiation.  [These course notes by Flaherty](https://web.archive.org/web/20190214233129/http://www.cs.rpi.edu/~flaherje/pdf/ode4.pdf) are in the ODE context, but are written in a general enough way that you can see the applicability to other problems, and they discuss adaptive error estimation briefly at the end.  The [Richardson.jl package](https://github.com/JuliaMath/Richardson.jl) in Julia implements the algorithm in a very general setting, and its documentation includes a number of examples; it's used by the [Romberg.jl package](https://github.com/fgasdia/Romberg.jl) for Romberg integration and by the [FiniteDifferences.jl](https://github.com/JuliaDiff/FiniteDifferences.jl) package for extrapolating finite-difference approximations.   In Python, I'm not currently aware of any general-purpose implementation (though there are implementations in the context of e.g. derivatives or integration).

## Lecture 12 (Feb 28)

* [pset 3 solutions](psets/pset3sol.ipynb)
* [pset 4](psets/pset4.ipynb): due Friday, March 7

Root finding is the problem of solving $f(x) = 0$.   For linear functions, this is easy.  For scalar polynomial functions, it's easy for degree 2 (using the [quadratic formula](https://en.wikipedia.org/wiki/Quadratic_formula)); for degree > 4 it is [impossible](https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem) to have a closed-form expression, but there are nowadays "easy" methods involving [eigenvalues of companion matrices](https://en.wikipedia.org/wiki/Companion_matrix).   However, for more general nonlinear functions there is not such a straightforward approach to get the roots.  You are left with [iterative methods](https://en.wikipedia.org/wiki/Iterative_method): a procedure to obtain a sequence $x_k$ of values, starting from some "guess" $x_0$, that approaches the root (hopefully quickly).

The most famous such algorithm is [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method), which you probably learned in first-year calculus.  The key idea is to use the derivative $f'$ to approximately *linearize* the function around $x_k$: $f(x_k + \delta) \approx f(x_k) + f'(x_k) \delta$.  Setting this linearization to zero gives us a **Newton step** $x_{k+1} = x_k - f'(x_k)^{-1} f(x_k)$.

In calculus, it was enough to set up this algorithm, try it out, and see that it works pretty well (if you have a reasonable starting point).   In numerical analysis, we want to be more precise: how fast does it converge, once $x_k$ is close to the root?   If the exact root is $r$, and the error on the k-th step is $\epsilon_k = x_k - r$, we showed that $\epsilon_{k+1} = O(\epsilon_k^2)$.  This is called "quadratic convergence".  This is entirely different from "second-order" convergence!  We have used "second-order" convergence to mean that the error after $N$ steps is $O(1/N^2)$, but it turns out that quadratic convergence (nearly *doubling the number of correct digits on each step*) has an error after $N$ steps that is a [double exponential](https://en.wikipedia.org/wiki/Double_exponential_function) $O(a^{-b^{N}})$ for some constants $a,b$.

Gave a numerical demo in which we applied Newton's method to find square roots $r = \sqrt{a}$, corresponding to a root of $f(x) = x^2 - a$.   The Newton iteration simplifies in this case to $x_{k+1} = \frac{1}{2} \left(x_k + \frac{a}{x_k}\right)$, which turns out to have been [known in ancient Bablylon](https://en.wikipedia.org/wiki/YBC_7289).  See also [these notes](https://math.mit.edu/~stevenj/18.335/newton-sqrt.pdf).

Another important extension of Newton's method is to *systems* of $n$ nonlinear equations ${f}({x})={0}$ where $f,x,0$ are $n$-component vectors.  It turns out that the *same* linearization and the *same* Newton step $x_{k+1} = x_k - f'(x_k)^{-1} f(x_k)$ works, with the *same* quadratic convergence.  The only difference is that now $f'(x_k)$ is an $n \times n$ [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant).

Newton's method works well if you start with a guess that is "reasonably close" to the root.  For starting points that are far from the root, however, it can be wildly unpredictable in fascinating ways, leading to [Newton fractals](https://en.wikipedia.org/wiki/Newton_fractal).

**Further reading**: [FNC book, chapter 4](https://fncbook.com/overview-3), especially [section 4.3](https://fncbook.com/newton) on Newton's method.   A key class of methods that we *didn't* cover involve what to do when you don't have access to the derivative (or in the case of large nonlinear systems the whole Jacobian matrix might be too expensive to compute or even store).  In this case, there are many algorithms:
* [Fixed-point iteration](https://en.wikipedia.org/wiki/Fixed-point_iteration) attempts to solve $g(x)=x$ ($g(x) = f(x) + x$) by $x_{k+1} = g(x_k)$.  This may not converge at all unless $|g'(r)| < 1$, but it is the starting point for the much more robust and fast-converging [Anderson-accelerated fixed-point iteration](https://en.wikipedia.org/wiki/Anderson_acceleration), which itself is closely related to "quasi-Newton" methods.
* In 1d, [secant methods](https://en.wikipedia.org/wiki/Secant_method) are Newton-like methods that perform linearization (or higher-order) by interpolating two (or more) points.  Generalization to multiple dimensions yields [quasi–Newton methods](https://en.wikipedia.org/wiki/Quasi-Newton_method) such as [Broyden's method](https://en.wikipedia.org/wiki/Broyden%27s_method) and Anderson acceleration.
* [Newton–Krylov methods](https://en.wikipedia.org/wiki/Newton%E2%80%93Krylov_method) only require you to compute Jacobian–vector products(jvp's) $f'(x_k)\delta$ for given $x_k, delta$, which corresponds to a single [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative) — this is much cheaper than computing the entire Jacobian in high dimensions, at the price of using an iterative *linear* solver to find each Newton step.
Another important class of methods are [numerical continuation](https://en.wikipedia.org/wiki/Numerical_continuation) methods, which solve a *family* of root problems $f(x,\lambda) = 0$ for the roots $x(\lambda)$ at each value of the parameter $\lambda$.  Given a root at one $\lambda$, the simplest algorithm is to use this as the starting point for Newton's method to find the root at a nearby $\lambda$, but there are much more sophisticated methods such as pseudo-arclength continuation.  A modern implementation of numerical continuation can be found in, for example, the [BifurcationKit.jl package](https://bifurcationkit.github.io/BifurcationKitDocs.jl/dev/) in Julia.  There are many books and reviews on this topic, e.g. [Herbert B. Keller, *Lectures on Numerical Methods in Bifurcation Problems* (Springer, 1988)](https://mathweb.tifr.res.in/sites/default/files/publications/ln/tifr79.pdf).

## Lecture 13 (Mar 3)

* lecture notes/code: (One Note)[https://mitprod-my.sharepoint.com/personal/qiqi_mit_edu/_layouts/15/Doc.aspx?sourcedoc={24321bd2-8c69-451a-b0cc-c2b42aed5743}&action=view&wd=target%28B.%20Initial%20Value%20Problems%2F20250303.one%7C6663d749-3747-45de-bb34-f8393732d800%2F%29&wdorigin=717]

Numerical methods for ordinary differential equations (ODEs).  Introduced the concept of an [initial value problem](https://en.wikipedia.org/wiki/Initial_value_problem) $\frac{du}{dt} = f(u,t)$ given $u(0)$, and some numerical methods: the [forward Euler (explicit)](https://en.wikipedia.org/wiki/Euler_method) method, the [backward Euler (implicit)](https://en.wikipedia.org/wiki/Backward_Euler_method) method, and the [midpoint method](https://en.wikipedia.org/wiki/Midpoint_method).   Showed that forward Euler has $O(\Delta t)$ error and the midpoint method has $O(\Delta t^2)$ error.

Looked at an example problem $\frac{du}{dt} = -u$ with $u(0)=1$, which has the analytical solution $u(t) = e^{-t}$.  Numerically, forward Euler and the midpoint method both exhibit the expected convergence rates.  But even though the midpoint method *seems* better in terms of convergence rate, its actual error is much *larger* than forward Euler until $\Delta t$ gets very small.   We will see next time that it is suffering from an "instability" where the errors *grow exponentially* with time $t$, making the "constant coefficient" of the $O(\Delta t^2)$ exponentially large.

Moreover, we used the "stencil" algorithm from pset 1 to derive a *3rd-order* finite difference approximation, and found that the behaviour was even worse than the midpoint rule.  Even for a *fixed* time $t$, the solutions in this case *diverged* exponentially rather than converging as $\Delta t$ decreases.  Next lecture, we will see that this is a failure of "zero stability".

**Further reading**: [FNC book, chapter 6](https://fncbook.com/overview-5), sections 6.1–6.2.  FENA book, chapter 4 and section 4.1.  You can also find hundreds of other web pages and videos on these topics.  3Blue1Brown has an [entertaining introduction to the idea of a differential equation](https://www.youtube.com/watch?v=p_di4Zn4wz4).   And here is a nice [video about the history of numerical ODE solvers](https://www.youtube.com/watch?v=gdxYsVniOYo) talks about the pioneering contributions of [Katherine Johnson](https://en.wikipedia.org/wiki/Katherine_Johnson) and her portrayal in the 2016 film [*Hidden Figures*](https://en.wikipedia.org/wiki/Hidden_Figures).

## Lecture 14 (Mar 5)

* notes and code: (One Note)[https://mitprod-my.sharepoint.com/personal/qiqi_mit_edu/_layouts/15/Doc.aspx?sourcedoc={24321bd2-8c69-451a-b0cc-c2b42aed5743}&action=view&wd=target%28B.%20Initial%20Value%20Problems%2F20250305.one%7Cd1ccb4e9-d0d8-414c-98cd-f1392b3d5277%2F%29&wdorigin=717] (Code)[https://colab.research.google.com/drive/1fmyyPSp5gImrcj42wxxn3kBqTDavq5qJ?usp=sharing]

**Stability** of numerical ODE methods.  The term "stability" has special meanings in this context, different from other areas of numerical analysis where "stability" is mostly about the sensitivity of an algorithm to roundoff errors.  For ODE methods, "stability" refers to sensitivity to **truncation error** $\Delta t$ (the finite timesteps used to approximate $du/dt$).  In particular, there are three central concerns for any ODE method:

* **Consistency**: Do the **local truncation errors** of a *single* time step $u_{n+1} = \cdots$ go to zero as $O(\Delta t^{1+p})$ for $p > 0$?  That is, are we correctly approximating $\frac{du}{dt} - f(u,t)$ to some positive order $O(\Delta t^p)$?
* **Zero stability**: With a right-hand sided $f=0$, it is **zero-unstable** if a nonzero initial condition can *diverge* ($\Vert u_k \Vert \to \infty$) as the step $k \to \infty$ (with $\Delta t$ cancelling from the analysis for $f=0$, so this can be viewed as a divergence as $\Delta t \to 0$ for a fixed $t$).
* **Linear stability**: Later, we will *include* the right-hand side in the stability analysis by *linearizing* it when $u$ is close to a root of $f$, especially for [autonomous ODEs](https://en.wikipedia.org/wiki/Autonomous_system_(mathematics)) $f(u,t) = f(u)$.  Through this analysis, we will show that some methods diverge for a *fixed* $\Delta t$ as you increase the total time $t$, perhaps unless $\Delta t$ is sufficiently small (*conditional* stability).

An important result is the **Dahlquist equivalence** theorem (closely related to the [Lax (or Lax–Richtmyer) equivalence theorem](https://en.wikipedia.org/wiki/Lax_equivalence_theorem)):
* If a method is *both consistent and zero-stable*, then it is **("globally") convergent**: the approximate solution $\tilde{u}(t)$ approaches the exact solution $u(t)$ at any time $t$, as $\Delta t \to 0$, with a convergence rate $\Vert\tilde{u}(t) - u(t)\Vert = O(\Delta t^p)$ matching the local truncation error.

That is, there are only two ways an ODE method could go badly wrong: either you have a bug in your finite-difference approximation (it's *inconsistent* with the equation you are discretizing, i.e. not approximating the right equation), or the solution diverges (it's *unstable*) as $\Delta t \to 0$.   This is good news, in a way!  You can't have a consistent scheme that silently converges to the wrong answer — if it fails, it will "fail loudly" in an obvious way (diverging).

All three of the schemes (Euler, midpoint, and third-order) from last lecture were consistent (with $p = 1,2,3$ respectively).   But the third-order scheme was zero-unstable.   (The midpoint rule was zero-stable: its solutions converged as $\Delta t \to 0$ for a *fixed* time $t$, but it exhibited a different kind of instability that we will discuss next time: its solutions could diverge for fixed $\Delta t$ as the time $t$ increased.)  Besides numerical experiments, how does one analyze zero-stability?

The trick to analyze zero stability is to set $f=0$ and notice that the ODE scheme $`u_{n+1} = \cdots`$ is in the form of a [linear recurrence relation](https://en.wikipedia.org/wiki/Linear_recurrence_with_constant_coefficients): $u_{n+1}$ is a linear combination of $`u_k`$ for $k < n$, with constant ($k$-independent) coefficients.   By combining several $`u_k`$'s into a vector $`{x}_k`$, we can write this in **matrix form** as ${x}_{k+1} = A{x}_k$ for some matrix $`A`$.  Iterating this relation, we immediately see that $`{x}_{k+1} = A^k {x}_1`$, and so the question becomes: do **matrix powers** $A^k$ diverge or not?   This can be analyzed by looking at [eigenvalues λ](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of $A$, satisfying $`A{x}=\lambda {x}`$ for some eigenvectors ${x} \ne 0$.  For an eigenvector $`{x}`$, $`A^k = \lambda^k {x}`$, so it diverges if $|\lambda | > 1$.   If *any* of the eigenvalues has $|\lambda | > 1$, then $`A^k {x}_1`$ will diverge for *some* initial condition $`{x}_1`$, and the system is unstable.  So, to check zero-stability, we just need to write the scheme in terms of a matrix $A$, compute the eigenvalues of $A$ (computers are good at this), and make sure $|\lambda | \le 1$ for all eigenvalues.  (Technically, this assumes that the matrix is [diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix); in the *very rare* case of a non-diagonalizable or ["defective"](https://en.wikipedia.org/wiki/Defective_matrix) matrix, more care is required.)

Showed by this analysis that Euler is zero-stable (its $A$ is a $1 \times 1$ matrix with eigenvalue $1$), the midpoint rule is also zero stable (its $A$ is a $2 \times 2$ [exchange matrix](https://en.wikipedia.org/wiki/Exchange_matrix) with eigenvalues $\pm 1$), but the third-order scheme was unstable (it had an eigenvalue of $\approx -2.6$)

**Furher reading**: FNC book, [section 6.8: zero stability](https://fncbook.com/zerostability) (which used an alternative framing equivalent to eigenvalues, but without explicitly forming the matrix).  FENA book, chapter 4.2 (which used the eigenvalue formulation).  For the general problem of analyzing matrix powers and linear recurrences via eigenvalues, you may want to review some material from 18.06: Strang *Intro. to Linear Algebra* section 6.2, and [18.06 OCW lecture 22](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/resources/lecture-22-diagonalization-and-powers-of-a/) (diagonalization and matrix powers).

## Lecture 15 (Mar 7)

* notes and code: [One Note](https://mitprod-my.sharepoint.com/personal/qiqi_mit_edu/_layouts/15/Doc.aspx?sourcedoc={24321bd2-8c69-451a-b0cc-c2b42aed5743}&action=view&wd=target%28B.%20Initial%20Value%20Problems%2F20250307.one%7C57e8a4c4-7a40-40fb-b165-f8144f92a40c%2F%29&wdorigin=717) [Code](https://colab.research.google.com/drive/1fmyyPSp5gImrcj42wxxn3kBqTDavq5qJ?usp=sharing)
* [pset 4 solutions](psets/pset4sol.ipynb)
* [pset 5](psets/pset5.ipynb): due Friday March 14

**Linear stability analysis** of ODEs and discretization schemes.

* The *exact* ODE $\frac{du}{dt} = \lambda u$ has exponentially growing solutions for $\Re \lambda > 0$, and non-growing ("stable") solutions for $\Re \lambda \le 0$.   This analysis extends to linear autonomous ODEs $\frac{du}{dt} = A u$ where $A$ is a (diagonalizable) matrix, since we can just check each eigenvalue $\lambda$ of $A$.  (Later, we will also extend this analysis to nonlinear autonomous ODEs $\frac{du}{dt} = f(u)$ by approximately *linearizing* $f(u)$ around a root using the Jacobian of $f$.)
* When we *discretize* the ODE, can plug $\lambda u$ (or $Au$) in for the right-hand side, for a fixed $\Delta t$, and again use eigenvalues to analyze whether $u_k \approx u(k\Delta t)$ is growing or decaying with $k$.  (This reduces to "zero stability" analysis in the limit $\Delta t \to 0$, for which the right-hand-side disappears from the formula for $u_k$.)

In this way, we find that certain discretization schemes are **linearly stable** (non-growing $u_k$) only for certain values of $\lambda \Delta t$.

To begin with, we analyzed the *forward Euler* ("explicit") scheme $u_{k+1} = u_k + \Delta t f(u_k) = u_k + \Delta t (\lambda u_k)$ for $\frac{du}{dt} = \lambda u$, giving $u_{k} = (1 + \lambda \Delta t)^k u_0$.  Hence, it is stable for $|1 + \lambda \Delta t| \le 1$, which the interior of a *circle* of radius 1 in the complex $\lambda \Delta t$ plane centered at $\lambda \Delta t = -1$.  Hence:

* For $\frac{du}{dt} = -u$, $\lambda = 1$ so it is stable for $0 \le \Delta t \le 2$.  (This is why it performed so well numerically.)  (The fact that it is only stable for certain values of $\Delta t$ in this case is called **conditional stability**; note that this depends on the right-hand side of the ODE!)
* For $\frac{d^2 u}{dt^2} = -u$, we saw last time that this is equivalent to the $2 \times 2$ system $`\frac{d}{dt} \begin{pmatrix} u \\ v \end{pmatrix} = \underbrace{\begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}}_{A} \begin{pmatrix} u \\ v \end{pmatrix}`$, where $A$ has eigenvalues $\lambda = \pm i$.   In this case $|1 + \lambda \Delta t| > 1$ for *all* $\Delta t > 0$, and forward Euler is *linearly unstable*.   It is still *zero stable*, so it still converges as $\Delta t \to 0$ for any fixed time $t$!  But instead of oscillating solutions (for the exact ODE), we have solutions that are oscillating and slowly exponentially growing (more and more slowly as $\Delta t$ gets smaller, which allows it to converge).  Even though the solutions converge, people are often unhappy with ODE methods that generate exponential growth (not present in the exact ODE) as you run for longer and longer times!

Next, we analyzed the *backward Euler* ("implicit") scheme $u_{k+1} = u_k + \Delta t f(u_{k+1}) = u_k + \Delta t (\lambda u_{k+1})$.  This gives $u_{k} = \frac{1}{(1 - \lambda \Delta t)^k} u_0$, which is stable for $|1 - \lambda \Delta t| \ge 1$.  Notice the $\ge$ sign!  This is the *exterior* of a circle of radius 1 in the complex $\lambda \Delta t$ plane centered at $\lambda \Delta t = +1$, which is a *superset* of the stable region $\Re \lambda \ke 0$ of the *exact* ODE solutions.

* This is why people use implicit ODE schemes: they are often more complicated to implement, because $f(u_{k+1})$ appears on the right-hand-side, requiring you to solve for $u_{k+1}$ (which gets expensive as $f$ gets more complicated), but they tend to be *more stable* than explicit schemes.

**Further reading:** FENA book, section 4.3.

## Lecture 16 (Mar 10)

* notes and code: see links above

Analyzed the linear stability of the midpoint rule, and found that it was only stable for a small range of purely imaginary $\lambda \Delta t$ values.

To illustrate the utility of "implicit" schemes like backwards Euler, which are more difficult to implement and require a more expensive time-stepping procedure (solving a system of equations on each step, possibly even a nonlinear system), considered a simple example problem involving heat flow.  In a system where heat can flow quickly between some components (e.g. metals in contact) but slowly between other components (e.g. between metals and air), one obtains a ["stiff" ODE](https://en.wikipedia.org/wiki/Stiff_equation), characterized by a **large ratio of timescales (eigenvalues)**.

With forward Euler (or other explicit methods) in a stiff problem, a small $\Delta t$ is required to resolve the fast timescale in a stable way, but then the number of timesteps is large (the slow timescale divided by $\Delta t$).   With *backward* Euler, however, it remains stable even for very large $\Delta t$, much larger than the fast timescale (although then the fast dynamics can't be observed accurately).   This is why implicit schemes are attractive: in stiff systems, the more-expensive implicit timesteps are worth it because of the huge savings in the *number* of timesteps (from the larger $\Delta t$).

**Further reading:** The FENA book, section 4.10, has some further discussion of stiff systems. There are many sources online about methods for stiff equations and implicit ODE methods.  See e.g. [these course notes from MIT 18.337/6.338](https://book.sciml.ai/notes/09-Solving_Stiff_Ordinary_Differential_Equations/), which focuses mainly on ways to construct the Jacobian (to linearize the right-hand-side) and/or efficiently perform the implicit solves on each step for the nonlinear case.  The [book by Griffiths and Highham (2010)](https://link.springer.com/book/10.1007/978-0-85729-148-6), along with many other similar books on numerical ODEs, contains a wealth of information, at a much more formal level than this course.

## Lecture 17 (Mar 12)

* notes and code: see links above

More numerical ODE schemes:
* [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule_(differential_equations)) (implicit) — analyzed order of accuracy (= 2) and stability (Re λ ≤ 0).  For large Δt, it produces undesirable oscillations if λ is real and < 0, but for purely imaginary λ (oscillating ODEs) it has the nice property of "conserving energy" (oscillating solutions with no artificial numerical growth or dissipation).
* [BDF (backward difference) rule](https://en.wikipedia.org/wiki/Backward_differentiation_formula) (implicit) — analyzed an order-2 scheme, which has the same stability as trapezoidal rule, but does not introduce oscillations for real λ (while it introduces artifical dissipation for imaginary λ).
* **Multi-step methods** and [Runge–Kutta schemes](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods) — began talking about schemes that operate in multiple "stages": first they "estimate" the future u at an intermediate timestep (e.g. k+½) to plug into the right-hand-side f(u,t), and then exploit this to get an improved value for u at timestep k+1.  Gave an example of a second-order *explicit* scheme using this strategy.

**Further reading:** FNC books, sections 6.4 and 6.6.  FENA book section 4.6 (trapezoidal) and 4.8–4.9 (Runge–Kutta and multistep methods).

## Lecture 18 (Mar 14)

* [pset 5 solutions](psets/pset5sol.ipynb)
* [pset 6](psets/pset6.ipynb): due **11am** on Friday 3/21

The big picture of numerical linear algebra: amateurs think mostly about element-by-element algorithms oriented towards hand calculations, whereas practitioners think mostly in terms of **factorizations** (which give the *results* of these algorithms in terms of a product of simpler matrices, which allow use to re-use and reason about them):

* Instead of [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination) (or Gauss–Jordan / matrix inversion), we think about [LU factorization](https://en.wikipedia.org/wiki/LU_decomposition) $PA = LU$: $U$ is upper triangular (the result of Gaussian elimination), $L$ is lower triangular (a record of the elimination steps), and $P$ is a permutation (a re-ordering of the rows, which happens more frequently on computers than by hand).
* Instead of [Gram–Schmidt orthogonalization](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process), we think about [QR factorization](https://en.wikipedia.org/wiki/QR_decomposition) $A = QR$: $Q$ has orthonormal columns and $R$ is upper triangular.  (In practice, computers typically use a [Householder algorithm](https://en.wikipedia.org/wiki/Householder_transformation) rather than Gram–Schmidt).
* Instead of finding eigenvalues by looking for roots of [characteristic polynomials](https://en.wikipedia.org/wiki/Characteristic_polynomial), we think about [diagonalization](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) $A = X \Lambda X^{-1}$: the columns of $X$ are eigenvectors, and $\Lambda$ is a diagonal matrix of eigenvalues.  The most important case is real-symmetric matrices $A = A^T$ (or complex [Hermitian matrices](https://en.wikipedia.org/wiki/Hermitian_matrix)), where $X=Q$ (orthonormal eigenvectors, different from the result of QR!) and $A = Q \Lambda Q^T$.  Practical algorithms for diagonalization are *very* different from what you do by hand; the most important is the [QR algorithm](https://en.wikipedia.org/wiki/QR_algorithm) discovered around 1960.
* Other important factorizations include the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) $A = U \Sigma V^T$ (which is hugely important in practice, but unfortunately gets tacked on at the end of many introductory linear-algebra courses), the [Schur factorization](https://en.wikipedia.org/wiki/Schur_decomposition) $A = QTQ^T$ (generalizing diagonalization to an upper triangular $T$ whose diagonal entries are the eigenvalues), [Cholesky factorization](https://en.wikipedia.org/wiki/Cholesky_decomposition) $A = LL^T$ (equivalent to LU factorization in the special case of [symmetric positive-definite](https://en.wikipedia.org/wiki/Definite_matrix) $A$, but twice as fast), [Hessenberg factorization](https://en.wikipedia.org/wiki/Hessenberg_matrix), and others.

For this lecture, we focused on LU factorization, and in particular a few key points:

* Why is LU factorization equivalent to Gaussian elimination?  By a simple 3x3 example, reviewed how Gaussian elimination steps produce an upper-triangular matrix $U$ from $A$, but working backwards yields $A = LU$ where $L$ is lower triangular (with 1s on the diagonal) and is obtained "for free": it is simply a *record* of the elimination steps.  It is easy to see that the cost of elimination (hence LU factorization) scales as $O(m^3)$ for an $m \times m$ matrix.
    * More generally, the factorization is $PA = LU$, where $P$ represents row swaps.  Trying the same 3x3 example on a computer, we found that the computer performed row swaps ($P \ne I$) even though it didn't "have" to (no zero pivots were encountered).   It turns out that the computer swaps rows to **make the pivots as big as possible** (for each column, it looks for the entry with maximum magnitude), an algorithm called *partial pivoting*, and this is essential to reduce sensitivity to roundoff errors in cases where the matrix entries have very different magnitudes.
* How do we use an LU factorization?  If you are solving $Ax = b$ and have $A = LU$, then you can equivalently solve $L(Ux) = b$ by (1) let $Ux=c$ and solve $Lc = b$ for $c$ and (2) solve $Ux=c$ for $x$.  These two "triangular solves" are easily done by [forward and backsubstitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution).  Moreover, solving $Lc=b$ by forward substitution is *exactly equivalent* to performing the Gaussian elimination steps (from $A$) on $b$, which is typically done in hand calculations by "augmenting" the matrix $A$ with an extra column $b$.   The cost of each of these steps is $O(m^2)$, very similar to a matrix–vector multiplication.  (So, solving new right-hand sides is cheap once you have the LU factors.)

Another important point is that **having LU factors is as good as — or even better than — having the matrix inverse.**
* Computing a matrix inverse is **more than twice as costly** as getting the LU factors.  To find $A^{-1}$, you essentially solve $AX = I$ for $X = A^{-1}$: $m$ right-hand-sides given by the columns of $I$.  This involves *first* finding the LU factorization of $A$, with $O(m^3)$ cost, and *then* solving $m$ right hand sides, with an *additional* $m \times O(m^2) = O(m^3)$ cost for the triangular solves.   (This process is *equivalent* to the Gauss–Jordan algorithm you may have learned by hand.)  So, like LU, it is $O(m^3)$, but the "constant factor" is usually at least twice as bad.
* If the matrix is [sparse (mostly zero)](https://en.wikipedia.org/wiki/Sparse_matrix), then the LU factors are often sparse as well, and can be computed and used very efficiently by skipping the zeros. But the matrix *inverse* is almost never sparse, so you lose that advantage.  Gave the example of a [tridiagonal matrix](https://en.wikipedia.org/wiki/Tridiagonal_matrix), for which the LU factors are actually [bidiagonal](https://en.wikipedia.org/wiki/Bidiagonal_matrix) (without row swaps) and can be computed and used in $O(m)$ time … but the *inverse* is all nonzero, and has $O(m^2)$ cost.
* A good (but not universal) rule of thumb is **never compute matrix inverses**.  When you see $x = A^{-1} b$, **read it as "solve Ax=b in the best way you can"**.   For example, if $A=LU$, we could formally write $A^{-1} b = U^{-1} L^{-1} b$, but you wouldn't *compute* the inverses of these triangular matrices — you would compute $c = L^{-1} b$ by forward-subsitution on $Lc=b$, then compute $x = U^{-1} c$ by back-substitution on $Ux = c$.   If you need to repeatedly apply $A^{-1}$ to many vectors, just LU-factorize $A$ and re-use the LU factors.

LU factorizations in Julia can be computed with [`F = lu(A)`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.lu), which returns a ["factorization" object `F`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.LU) that contains the permutation $P$ (stored as an ordering `F.p`) and the $L,U$ factors.  You can then compute $A^{-1} b$ by `F \ b` (which does the permutation and 2 triangular solves).   By default, it uses partial pivoting: always permuting rows to maximize the magnitude of the pivot.  To compare to hand calculations (where you only do row swaps to avoid zero pivots), you can do `F = lu(A, RowNonZero())`, but never do this for "serious" work because it is "numerically unstable" and can cause roundoff errors to blow up.  If you do `x = A \ b` in Julia, it will "solve Ax=b in the best way it can", by default using LU factorization.

In Python, the analogue of `F = lu(A)` is [`scipy.linalg.lu`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html), the analogue of `F \ b` is [`scipy.linalg.lu_solve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html), and the analogue of `A \ b` is [`numpy.linalg.solve`](https://numpy.org/doc/2.2/reference/generated/numpy.linalg.solve.html).

**Further reading:** FNC book, section 2.4 on LU factorization (and section 3.3 on QR, and chapter 7).  [18.065 OCW lecture 2](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/resources/lecture-2-multiplying-and-factoring-matrices/), [Strang 18.06 lecture 4 on LU factorization](https://www.youtube.com/watch?v=5hO3MrzPa0A).  Matrix factorizations are discussed in every linear-algebra textbook at various levels of sophistication and practicality.

## Lecture 19 (Mar 17)

A brief survey of other topics in ODE methods:

* [Runge–Kutta schemes](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods): the general structure is a "tableau" of coefficients where you make a sequence of estimates $u((k+a)\Delta t)$ for coefficients $a \in (0,1]$, and then make a linear combination of these estimates to estimate $u((k+1)\Delta t)$.   Deriving this tableau is somewhat of an art form, and typically involves careful choices of simplifying assumptions.  People continue to discover new Runge–Kutta schemes!  For example, state of the art 5th order scheme was recently found by [Tsitorious (2010)](https://www.sciencedirect.com/science/article/pii/S0898122111004706).
    * [Adaptive schemes](https://en.wikipedia.org/wiki/Adaptive_step_size): similar to adaptive quadrature methods, typically high-order Runge-Kutta methods are "nested": using a subset of the same function values, one gets a *lower-order* estimate "for free", and compared to the high-order estimate this gives an **error estimate**.   In this way, they can **adaptively adjust Δt** until the error estimate obeys a prescribed tolerance.
* [Boundary-value problems](https://en.wikipedia.org/wiki/Boundary_value_problem): instead of providing a purely initial condition $u(0)$, here one specifies a *final* condition $u(T)$, or more generally a *mix* of constraints on $u(0)$ and $u(T)$ (where the total number of constraints equals the dimension of $u$).  One approach to solving such problems is to **reduce it to root-finding**: one tries to solve for $u(0)$ that satisfies the constraints on $u(T)$, sometimes called a ["shooting" method](https://en.wikipedia.org/wiki/Shooting_method).
    * To apply Newton's method, one then needs the Jacobian of $u(T)$ with respect to the initial conditions $u(0)$, which is a special example of the important problem of **differentiating ODE solutions** (with respect to parameters of the equations and/or initial conditions).   Not only is this useful for boundary-value problems, but it is important for [sensitivity analysis](https://en.wikipedia.org/wiki/Sensitivity_analysis) and quantifying uncertainties, and has become widely used for **optimization** of ODE solutions (e.g. to fit experimental data or to improve some other objective).   See the links below.
* [Differential–algebraic equations (DAEs)](https://en.wikipedia.org/wiki/Differential-algebraic_system_of_equations) — a DAE couples an ODE $\frac{du}{dt} = f(u,v,t)$ with a set of (possibly nonlinear equations) $g(u,v,t)=0$ that have to be solve simultaneously with the ODE to find additional unknowns $v(t)$.   Sometimes, you can explicitly solve $g=0$ to find $v$, and eliminate these unknowns, giving just an ODE in $u$, but in other cases this is impractical or inconvenient.  DAE algorithms simultaneously evolve $u$ from the ODE and $v$ from the "algebraic" equations.  In some ways these are analogous to stiff ODE solvers, because the $v$ equations respond "infinitely quickly" to changes in $u$.
* [Integro-differential equations](https://en.wikipedia.org/wiki/Integro-differential_equation) and [delay differential equations (DDEs)](https://en.wikipedia.org/wiki/Delay_differential_equation) are like ODEs but $du/dt$ depends explicitly not just on $u(t)$ but *also* on the solution $u(t')$ at times $t' < t$ in the past — either via a continuous integral or via a discrete sum of terms.   They have specialized ODE solver methods, which in principle are similar to ordinary ODE schemes but need to additionally keep track of (and interpolate) the past solutions as needed.
* [Stochastic differential equations (SDEs)](https://en.wikipedia.org/wiki/Stochastic_differential_equation) are like ODEs where the right-hand side includes a random "noise" term.   (Even *defining* precisely what this means requires a new form of calculus: **stochastic calculus**.)  Don't just plug random numbers into the right-hand-side of an ODE scheme!   There are a variety of specialized SDE methods.  Unfortunately, these methods are typically low-order: it difficult to obtain a high-order "strong" SDE scheme that produces the correct distribution of solutions.  But if you only care about the expected value ("average") of the solutions, there are higher-order "weak" SDE methods to accomplish this.   See e.g. the [SDE solver algorithms](https://docs.sciml.ai/DiffEqDocs/stable/solvers/sde_solve/) page in Julia's sophisticated DifferentialEquations.jl package.

**Further reading (ODE methods):** There are many books that give a more sophisticated treatment of numerical methods for ODEs and related problems, such the [textbook by Butcher](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119121534) or the books by [Hairer](https://en.wikipedia.org/wiki/Ernst_Hairer)'s on [non-stiff (1993)](https://link.springer.com/book/10.1007/978-3-540-78862-1) and [stiff/DAE (1996)](https://link.springer.com/book/10.1007/978-3-642-05221-7) algorithms, but the field continues to evolve with new algorithms.  Fortunately, there are now many full-featured ODE solver packages, available in a variety of languages and often free/open-source, that implement sophisticated algorithms for all of the above, and more!   A useful [2017 overview of available ODE software packages](https://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/) gives a useful comparison of what features and algorithms they implement — written by [Dr. Chris Rackauckas](https://www.stochasticlifestyle.com/) (then at MIT), who developed the state-of-the-art [DifferentialEquations.jl suite](https://docs.sciml.ai/DiffEqDocs/stable/) in Julia.

**Further reading (differentiating ODE solutions):**  Computing derivatives of ODE solutions (and functions thereof) with respect to parameters or initial conditions was introduced in our IAP class [18.063: Matrix Calculus](https://github.com/mitmath/matrixcalc/) — see the [course notes (chapter 9)](https://arxiv.org/abs/2501.14787)
and [lecture video 5 (forward mode)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b777873f-f6a0-4309-bf7d-b26a00842046) and [lecture video 6 (reverse mode)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0199e13d-4812-49d7-9a4a-b27100840437).   This has become an increasingly important topic because of the growth of optimization and machine learning, and it becomes critically important for large-scale problems to know the pros and cons of "forward" and "reverse" mode algorithms.    A recent review article is [Sapienza et al. (2024)](https://arxiv.org/abs/2406.09699), and a classic reference on adjoint-method (reverse-mode/backpropagation) differentiation of ODEs (and DAEs) is [Cao et al (2003)](https://epubs.siam.org/doi/10.1137/S1064827501380630) ([pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.455&rep=rep1&type=pdf)).  See also the [SciMLSensitivity.jl package](https://docs.sciml.ai/SciMLSensitivity/stable/) for sensitivity analysis with Chris Rackauckas's amazing [DifferentialEquations.jl software suite](https://diffeq.sciml.ai/stable/) for numerical solution of ODEs in Julia, along with his [notes from 18.337](https://rawcdn.githack.com/mitmath/18337/7b0e890e1211bfa253782f7862389aeaa321e8d7/lecture11/adjoints.html).  There is a nice YouTube [lecture on adjoint sensitivity of ODEs](https://www.youtube.com/watch?v=k6s2G5MZv-I), again using a similar notation.   A *discrete* version of this process is [adjoint methods for recurrence relations](https://math.mit.edu/~stevenj/18.336/recurrence2.pdf) (MIT course notes), in which case one obtains a reverse-order "adjoint" recurrence relation.

## Lecture 20 (Mar 19)

Back when we were solving least-square problems (minimizing $\Vert Ax-b\Vert_2$), we noted that the usual "normal equations" approach of solving $A^T A x = A^T b$ is not robust, because it squares the condition number of $A$.  A more robust approach is to use a "QR factorization" $A = QR$ to yield $R x = Q^T b$, which avoids squaring the condition number because a factor of $R$ has been cancelled *analytically*.  In this lecture, we consider this QR factorization in more detail.  It turns out that there are (at least) two variants of QR factorization:

The "thin" QR factorization $A = \hat{Q}\hat{R}$ factorizes a "tall" $m \times n$ matrix ($m \ge n$) $A$ into a "tall" $m \times n$ matrix $\hat{Q}$ with orthonormal columns ($\hat{Q}^T \hat{Q} = I$) multiplied by an upper-triangular $n \times n$ matrix $\hat{R}$ (which is invertible if $A$ has independent columns).

* The "meaning" of this factorization is that not only do the columns of $\hat{Q}$ form an orthonormal basis for column space $C(A)$ (the span of the columns of $A$), but the **first k** columns of $\hat{Q}$ are an orthonormal basis for the **first k** columns of $A$.  This is the meaning of the triangular structure of $\hat{R}$.

Understanding the above structure of the QR factorization leads immediately to the [Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) where one forms the columns $q_1, q_2, \ldots, q_n$ of $\hat{Q}$ by orthonormalizing the columns $a_1, a_2, \ldots, a_n$ of $A$ one by one.   Unfortunately, roundoff errors can lead to a loss of orthogonality and "numerical instability" if Gram–Schmidt is not carried out and *used* very carefully.

Instead, computers typically apply a different approach, inspired by a different form of the QR factorization.  The "full" QR factorization $A = QR$ factorizes a "tall" $m \times n$ matrix ($m \ge n$) $A$ into a **square** $m \times m$ matrix $Q$ with orthonormal columns ($Q^T Q = I$) — an [orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix) $Q^T = Q^{-1}$ which can be interpreted as an $m$-dimensional [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix) — multiplied by an upper-triangular "tall" $m \times n$ matrix $R$.   The first $n$ columns of $Q$ are $\hat{Q}$, and the remaining $m-n$ colums are an orthonormal basis for $C(\hat{Q})^{\perp} = C(A)^{\perp} = N(A^T)$ (the left nullspace).  The first $n$ rows of $R$ are $\hat{Q}$, and the remaining rows are zero.   That is, the full QR factorization is obtained by appending additional orthonormal columns to $\hat{Q}$ to make it square, multiplied by additional *zero* rows appended $\hat{R}$ so that the result is unchanged.  At first glance, this seems like more work, and more storage, than the thin QR factorization, but it turns out that this is not the case!

Gram–Schmidt works by turning $A$ into $\hat{Q}$.  The popular **Householder QR** algorithm instead turns $A$ into $R$ (upper-triangular) by multipling it on the left with a sequence of orthogonal "reflection" matrices $R = Q_{n-1} \cdots Q_2 Q_1 A$, so that $Q = Q_1^T Q_2^T \cdots Q_{n-1}^T$.  Each of these $Q_k$ matrices turns *one* column of the matrix into upper triangular form (introducing zeros below row $k$), and it turns out that there is a simple formula called a "Householder reflector" to construct such a matrix.   Moreover, each $Q_k$ can be represented by storing *one* vector $v$ (the "reflector"), and $Q_k x = Q_k^T x$ can be computed by $x - 2v(v^T x)$, for $O(m)$ cost and storage.     This leads Householder QR to require $O(mn^2)$ work to find all the reflectors and form $R$, and only $O(mn)$ storage to store $Q$ *implicitly* as a set of reflectors.   We don't need to store the *entries* of the $m \times m$ matrix $Q$, as long as we can multiply $Q$ or $Q^T$ by vectors quickly!

* Just because we write a matrix $M$ in linear algebra doesn't mean that we need to *store* it as a matrix, i.e. store a 2d array of its entries *explicitly*.   We often just need a way to **compute matrix-vector products** $Mx$ quickly for any given $x$, i.e. an algorithm implementing the *linear operator* $x \mapsto M x$.  An explicit matrix is just one possible representation of a linear operator, and not always the best one.  Examples:
    * The identity matrix $I$ rarely needs to be stored explicitly, since we can multiply it by vectors with no work at all!
    * Matrix inverses $A^{-1}$ should rarely be computed explicitly, because we can compute $A^{-1} b$ for any $b$ by solving $Ax = b$ by some other method, e.g. using the LU factors.
    * For matrices that are [sparse](https://en.wikipedia.org/wiki/Sparse_matrix) (mostly zero), a 2d array of entries is very wasteful because you are storing lots of zeros.  Instead, specialized sparse-matrix data structures store only the nonzero entries, and can use these nonzero entries to multiply by vectors quickly.   This is especially important for large-scale linear algebra, where the dimensions can be millions or more — **huge matrices are almost never stored explicitly as 2d arrays**.
    * QR factorization returns an implicit representation of $Q$ as a collection of Householder reflector operations.
* Given an algorithm to act $M$ on a vector, you *can* compute the entries of $M$ explicitly simply by computing $M I = M$: multiplying $M$ by the columns of $I$.  But often this is a waste of effort, as in the examples above.

**Further reading:** FNC book, sections [3.3](https://fncbook.com/qr) [3.4](https://fncbook.com/house). and [Householder QR notes from 18.335 at MIT](https://github.com/mitmath/18335/blob/spring21/notes/lec6handout6pp.pdf).  Any textbook on numerical linear algebra will cover QR factorization, e.g. [the book by Trefethen and Bau](https://www.stat.uchicago.edu/~lekheng/courses/309/books/Trefethen-Bau.pdf), section II.

## Lecture 21 (Mar 21)

* [pset 6 solutions](psets/pset6sol.ipynb)

Numerical methods for eigenvalue problems $Ax = \lambda x$ with an $n \times n$ matrix $A$, or equivalently for diagonalization $A = X \Lambda X^{-1}$.

* Can we find all the eigenvectors and eigenvalues with $O(n^3)$ cost (similar to other matrix factorizations)?  **Yes** (to a good approximation), but the algorithms are quite tricky — the first practical algorithm (the ["QR" algorithm](https://en.wikipedia.org/wiki/QR_algorithm)) was discovered around 1960.  Here, we'll only give some of the *ideas* that go into practical algorithms, which are too complicated to describe fully in a short lecture.

* *Should* we find all eigenvectors and eigenvalues?   The problem is that many matrices are close to being non-diagonalizable (["defective"](https://en.wikipedia.org/wiki/Defective_matrix)): their $X$ matrix is almost singular (ill-conditioned).  This makes *working* with diagonalization susceptible to a huge amplification of roundoff errors — unless you are very careful, diagonalizing a nearly defective matrix can be nearly useless.    However, if a matrix is **real-symmetric** $A = A^T$ (more generally [Hermitian](https://en.wikipedia.org/wiki/Hermitian_matrix), or even more generally ["normal"](https://en.wikipedia.org/wiki/Normal_matrix)), then the eigenvectors are *orthogonal*.  In this case, we can choose $X = Q$ (an orthogonal matrix), and $A = Q \Lambda Q^T$ is always well-conditioned (if we can solve the eigenproblem).

In introductory linear algebra, you learn how to solve *tiny* (often just 2×2) eigenproblems by first finding the [characteristic polynomial](https://en.wikipedia.org/wiki/Characteristic_polynomial) $p(\lambda) = \det(A - \lambda I) = (-\lambda)^n + \cdots + \det A$, then you find the eigenvalues from the roots $p(\lambda) = 0$ (e.g. by the quadratic formula for $n=2$).   This encounters three practical problems if we try to generalize it to arbitrary $n$:

1. How do we find $p(\lambda)$?  The generalization of your $2\times 2$ determinant formula is the [Leibniz formula](https://en.wikipedia.org/wiki/Leibniz_formula_for_determinants), which has $O(n!)$ cost: worse than exponential!   You can take the determinant of one matrix as a *number* in $O(n^3)$ operations by first doing Gaussian elimination (LU factorization), and then using $\det A = \det(L) \det(U) (-1)^{\text{no. row swaps}} = (\text{product of pivots}) (-1)^{\text{no. row swaps}}$, but it is not obvious how to use this to get all the polynomial coefficients.  It turns out that it *is* possible to get the characteristic polynomial coefficients in $O(n^3)$ time with some cleverness, but it is useless because…

2. Monomials are a terrible basis for high-degree polynomials.  We already saw this with polynomial interpolation, and a similar problem arises here.   It turns out that the roots of a polynomial are *exponentially* sensitive (in the degree $n$) to perturbations in the coefficients.  So, if you make even a tiny roundoff error in finding the coefficients, the resulting roots will be garbage.  A famous example of this is [Wilkinson's polynomial](https://en.wikipedia.org/wiki/Wilkinson%27s_polynomial) $(x-1)(x-2)\cdots(x-20)$, which is a degree-20 polynomial with roots 1,2,…,20: if you perturb the coefficients randomly in the 10th significant digit, the resulting roots change completely (not even 1 digit is correct in many cases).  We could maybe do better with a better polynomial basis, such as Chebyshev polynomials, but run into a third problem:

3. There is no analogue of the quadratic formula for polynomials of degree ≥ 5: there is no way to get the *exact* roots in a *finite* number of {±,×,÷,ᵏ√} operations.  This fact is one of the great triumphs of 19th-century mathematics, the [Abel–Ruffini theorem](https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem).   We can **still find the roots to any desired accuracy**, but the algorithms are of a fundamentally different nature than previous linear-algebra algorithms: they must be **iterative algorithms** that *approach* the eigenvalues but never exactly reach them.

We have already learned one possible iterative algorithm for root finding, **Newton's method**.  Given $f(\lambda) = \det(A - \lambda I)$ and an initial guess for $\lambda$, recall that we just update our guess repeatedly with the Newton step $\lambda \longleftarrow \lambda - f(\lambda) / f'(\lambda)$.  But how do we compute $f'(\lambda)$, the derivative of the determinant?  It turns out that there is a simple formula, from the fact that the determinant is the *product* of the eigenvalues:

$$
f(\lambda) = (\lambda_1 - \lambda) (\lambda_2 - \lambda) (\lambda_3 - \lambda) \cdots (\lambda_n - \lambda)
$$

where $\lambda_k$ are the eigenvalues of $A$ (hence $\lambda_k - \lambda$ are eigenvalues of $A - \lambda I$).   It follows by the product rule that

$$
f'(\lambda) = -(\lambda_2 - \lambda) \cdots (\lambda_n - \lambda) - (\lambda_1 - \lambda) (\lambda_3 - \lambda) \cdots (\lambda_n - \lambda) - \cdots = f(\lambda) \sum_k \frac{1}{\lambda_k - \lambda}
$$

But recalling that the [trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) of a matrix is the *sum* of the eigenvalues, we can identify the final sum as $\text{tr}\left[(A - \lambda I)^{-1}\right]$.  Hence the Newton step is:

$$
\lambda \longleftarrow \lambda - \frac{f(\lambda)}{-f(\lambda)\text{tr}\left[(A - \lambda I)^{-1}\right]} = \lambda + \frac{1}{\text{tr}\left[(A - \lambda I)^{-1}\right]}
$$

which is a simple formula only involving the trace of an inverse (the determinant cancelled!).   We tried this in Julia and saw that it indeed converges rapidly to an eigenvalue, given a decent initial guess.   This is the starting point for some practical eigenvalue algorithms, in cases where you know roughly where the eigenvalues of interest lie (e.g. because you are tracking an eigenvalue as the matrix changes), and there are many generalizations of these ideas!

Another simple iterative method is the [power iteration](https://en.wikipedia.org/wiki/Power_iteration): if you start with a random $x$ and multiply by $A$ repeatedly, then the resulting $A^k x$ converges to an eigenvector with the biggest $|\lambda|$.  Typically, you normalize at each step to avoid over/underflow.   Given the resulting eigenvector $q_1$, you can obtain the corresponding eigenvalue in a number of ways, most accurately by the ["Rayleigh quotient"](https://en.wikipedia.org/wiki/Rayleigh_quotient) $q_1^T A q_1$.   How do we find any *other* eigenvector?  For a real-symmetric matrix $A$, we can use the fact that the eigenvectors are *orthogonal*: generate another random $x$, and again repeatedly multiply by $A$, but at each step project orthogonal to $q_1$, i.e. $x \longleftarrow x - q_1 (q_1^T x)$.  This will lead to $q_2$, an eigenvector of the *second* biggest $|\lambda|$.  To find the third biggest $|\lambda|$, we can orthogonalize against both $q_1$ and $q_2$, and so forth.  (This repeated-projection algorithm is called "deflation" because it effectively shrinks the dimension of the space, projecting out one eigenvector at a time.)

There are many eigenvalue algorithms built on top of the power algorithm, from "shifted inverse iteration" and [Rayleigh quotient iteration](https://en.wikipedia.org/wiki/Rayleigh_quotient_iteration) to the [Arnoldi](https://en.wikipedia.org/wiki/Arnoldi_iteration) and [Lanczos](https://en.wikipedia.org/wiki/Lanczos_algorithm) methods.   But for general "dense" matrices, the most famous and perhaps most mysterious is the [QR algorithm](https://en.wikipedia.org/wiki/QR_algorithm).  Given a matrix $A$, we find the QR factors $A = QR$ and then form a new matrix $B = RQ$.  Repeating this process, for $A=A^T$, converges to a diagonal matrix of eigenvalues sorted in descending order by $|\lambda|$.  This sorting is a clue that QR iteration is subtly related to the power method, combined with orthogonalization, but explaining *how* this works takes quite a bit of effort.   It turns out that this is precisely the algorithm used by `eigvals(A)` in Julia, Numpy, and more.

One problem with the Newton and QR methods as written above is that *each step* requires $O(n^3)$ work.  It turns out that there is a way to "preprocess" the matrix into a [Hessenberg factorization](https://en.wikipedia.org/wiki/Hessenberg_matrix), which costs $O(n^3)$ only *once*, so that subsequent Newton or QR steps cost only $O(n)$ work (for real-symmetric $A$, or $O(n^2)$ work for general $A$).

**Further reading:**  Given a degree-$n$ polynomial, the most common way to find its roots is to put the coefficients into a ["companion matrix"](https://en.wikipedia.org/wiki/Companion_matrix) and find the eigenvalues(!), but there is a rich literature on [other methods for polynomial root-finding](https://en.wikipedia.org/wiki/Polynomial_root-finding).  Here is an [18.06 Julia notebook on eigenvalues and polynomials](https://nbviewer.org/github/stevengj/1806/blob/fall18/lectures/Eigenvalue-Polynomials.ipynb).   Derivatives (and gradients!) of determinants were covered more generally in [18.063: Matrix Calculus (lecture 3)](https://github.com/mitmath/matrixcalc/?tab=readme-ov-file#lecture-3-jan-17) and in [chapter 7 of the course notes](https://arxiv.org/abs/2501.14787).  Variations on Newton's method are especially common in the context of [nonlinear eigenproblems](https://en.wikipedia.org/wiki/Nonlinear_eigenproblem) where you are finding roots of $\det M(\lambda) = 0$.   QR iteration and other eigenvalue algorithms are discussed in more detail in the graduate class [18.335 (lectures 16–18)](https://github.com/mitmath/18335/tree/spring21?tab=readme-ov-file) and e.g. in the accompanying [textbook by Trefethen and Bau](https://www.stat.uchicago.edu/~lekheng/courses/309/books/Trefethen-Bau.pdf).
