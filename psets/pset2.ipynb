{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed51647",
   "metadata": {},
   "source": [
    "# 18.S190/6.S090 Problem Set 2\n",
    "\n",
    "Due 11:59pm, Friday February 21."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bf8ab-99e0-4356-9624-fd0cdfe769a9",
   "metadata": {},
   "source": [
    "## Problem 1 (2+2+2+3+2+2+2 points)\n",
    "\n",
    "Consider a linear least-square fitting (regression) problem in which your input data $x$ lies within the narrow domain $[100000, 100001]$. You want to fit a model of the form:\n",
    "\n",
    "$$\n",
    "y \\approx \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "You can represent this model in two different bases:\n",
    "1. The **standard monomial basis**: $\\{1, x\\}$,\n",
    "2. A **shifted basis**: $\\{1, x - 100000.5\\}$.\n",
    "\n",
    "For $N$ data points $\\{(x_i, y_i)\\}_{i=1}^N$, the mean squared error (MSE) loss function for both is written as:\n",
    "\n",
    "$$\n",
    "L(\\beta_0, \\beta_1) = \\frac{1}{N} \\sum_{i=1}^N \\bigl(\\beta_0 + \\beta_1 x_i - y_i\\bigr)^2,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\tilde{L}(\\tilde{\\beta}_0, \\tilde{\\beta}_1) = \\frac{1}{N} \\sum_{i=1}^N \\Bigl(\\tilde{\\beta}_0 + \\tilde{\\beta}_1 (x_i - 100000.5) - y_i\\Bigr)^2.\n",
    "$$\n",
    "\n",
    "### Tasks (hand calculations)\n",
    "\n",
    "1. Write the two loss functions as $L = \\frac{1}{N} \\Vert A\\beta - y \\Vert^2$ and $\\tilde{L} = \\frac{1}{N} \\Vert \\tilde{A}\\tilde{\\beta} - y \\Vert^2$, respectively.  What are the matrices $A$ and $\\tilde{A}$?\n",
    "\n",
    "2. Give a formula writing $\\tilde{A}$ as some matrix $B$ times $A$ (on the left or right?).  Denote the least-squares solutions (optimal $\\beta$) by $\\beta_*$ and $\\tilde{\\beta}_*$, respectively (e.g. $\\beta_*$ satisfies the normal equations $A^T A \\beta_* = A^T y$, from class).  Give a formula relating $\\beta_*$ and $\\tilde{\\beta}_*$ in terms of your matrix $B$.\n",
    "\n",
    "3. Write the gradients $\\nabla_\\beta L$ and $\\nabla_\\tilde{\\beta} \\tilde{L}$ of the two loss functions, in terms of the matrices and vectors from the previous part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f1757",
   "metadata": {},
   "source": [
    "### Tasks (computer)\n",
    "\n",
    "Generate some random data from a \"true\" model $y(x) = 1 + 10x$ plus noise, in Julia or Python, from 200 equally spaced points $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia:\n",
    "x = range(100000, 100001, length=200)\n",
    "y = @. 1 + 10x + 0.1 * randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f21846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "import numpy as np\n",
    "x = np.linspace(100000, 100001, 200)\n",
    "y = 1 + 10*x + 0.1 * np.random.randn(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40911163",
   "metadata": {},
   "source": [
    "4. Construct your matrices $A$ and $\\tilde{A}$, and compute the least-squares solutions $\\beta_*$ and $\\tilde{\\beta}_*$, using `β_star = A \\ y` in Julia or `β_star = numpy.linalg.lstsq(A, y)` in Python, and similarly for $\\tilde{A}$.  Check that $\\beta_*$ and $\\tilde{\\beta}_*$ are related by your matrix $B$ as predicted above (up to many digits, i.e. up to roundoff errors).  Plot the two fits (as lines) along with the data (as dots) to make sure they looks okay.  (The two lines should look identical. Why?)\n",
    "\n",
    "5.  Suppose we take a single gradient-descent step $\\beta_1 = \\beta_0 - s\\left.\\nabla L\\right|_{\\beta_0}$ starting at a \"guess\" $\\beta_0 = [0, 0]$, with a learning rate $s$, and similarly for $\\tilde{L}$.   If you set $s = 10^{-7}$ in both $L$ and $\\tilde{L}$, how much do the two loss functions decrease by, and how much does the error $\\Vert \\beta_k - \\beta_* \\Vert$ (or $\\Vert \\tilde{\\beta}_k - \\tilde{\\beta}_* \\Vert$) decrease by?\n",
    "\n",
    "6. Another way to characterize gradient descent is how well the \"downhill\" direction $-\\nabla L$ points towards the optimum $\\beta_*$.  Compute the *angle* (via the [dot product formula](https://en.wikipedia.org/wiki/Dot_product#Geometric_definition)) between $\\left. -\\nabla L \\right|_{\\beta_0}$ and $\\beta_* - \\beta_0$, and similarly between $\\left. -\\nabla \\tilde{L} \\right|_{\\tilde{\\beta}_0}$ and $\\tilde{\\beta}_* - \\tilde{\\beta}_0$.  Which one points more directly at the optimum?   Is this consistent with your observations from the previous part?\n",
    "\n",
    "7. Compute the condition numbers of the two matrices via `cond(A)` in Julia (do `using LinearAlgebra` first) or `numpy.linalg.cond(A)` in Python.   It turns out to be quite a general result that badly conditioned matrices lead to slow convergence in gradient descent (as well as exacerbating roundoff errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39926afb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Problem 2 (5+5+5 points)\n",
    "\n",
    "If you do degree-$n$ polynomial interpolation of $f(x)$ using Chebyshev points, i.e. the cosine of equally spaced points on $[-1,1]$ (`numpy.cos(numpy.linspace(-1,1,n+1))` in Python or `cos.(range(-1,1,length=n+1))` in Julia), then it turns out that the polynomial interpolant converges *exponentially* quickly with $n$ is $f(x)$ is an [\"analytic\" function](https://en.wikipedia.org/wiki/Holomorphic_function), i.e. if it is infinitely differentiable (\"smooth\") and has a convergent Taylor series.  More precisely\n",
    "$$\n",
    "    \\max_{x \\in [-1,1]} |f(x) - p(x)| \\leq C K^{-n},\n",
    "$$\n",
    "for some constants $C > 0$ and $K > 1$ where $p(x)$ is the degree-$n$ interpolating polynomial through our $n+1$ Chebyshev points.\n",
    "\n",
    "However, the convergence is not as rapid if $f(x)$ is not so smooth, e.g. if it has a discontinuity in one of its derivatives.  In this problem, you will investigate how this convergence rate relates to the smoothness of the function being interpolated.\n",
    "\n",
    "You will want to use the $n+1$ Chebyshev points as defined above, and to avoid numerical polynomials you should use a basis of Chebyshev polynomials $T_k(x)$ in your \"Vandermonde\" matrix, for $k = 0,\\ldots,n$.  $T_k(x)$ can be computed in Python by `numpy.cos(k * numpy.acos(x))`, or in Julia by `cos.(k .* acos.(x))`, for an array of points `x` in $[-1,1]$. (There are more efficient methods, but that isn't a concern here.)\n",
    "\n",
    "Consider the family of functions $f_m(x) = |x|^m$ for odd integers $m=1,3,5,\\ldots$.\n",
    "\n",
    "1. **Differentiability:** How many continuous derivatives over $[-1,1]$ does $f_m$ possess for $m=1,3,5,\\ldots$? (Your answer should be a function of $m$.)\n",
    "\n",
    "2. **Polynomial Interpolation:** In compute the polynomial interpolant using $n+1$ second-kind Chebyshev nodes in $[-1,1]$ for $n+1 = 10, 20, 30, \\dots, 100$. At each value of $n$, compute the max-norm error $\\max_x |p(x) - f_m(x)|$ evaluated for 100000 equally-spaced of $x$. Using a single log–log graph, plot the error as a function of $n$ for six values $m = 1, 3, 5, 7, 9, 11$.\n",
    "\n",
    "3. **Asymptotic Hypothesis:** Based on the results of parts (a) and (b), form a hypothesis about the asymptotic convergence rate of the error $n \\to \\infty$ depending on the number $\\ell$ of continuous derivatives.  (e.g. an *incorrect* answer would be that the error goes as some constant times $\\ell^{-n}$, but your answer should be some formula like this.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacd359",
   "metadata": {},
   "source": [
    "## Problem 3 (10 points)\n",
    "\n",
    "Kepler found that the orbital period $\\tau$ of a planet depends on its mean distance $R$ from the sun according to:\n",
    "\n",
    "$$\n",
    "    \\tau = c R^{\\alpha}\n",
    "$$\n",
    "\n",
    "for a simple rational number $\\alpha$. Perform some form of least-squares fit (or fits) to data from the following table in order to determine the most likely simple rational value of $\\alpha$.  (*Hint*: what does $\\log \\tau$ look like in Kepler's hypothesis?)\n",
    "\n",
    "| Planet   | Distance from Sun (Mkm) | Orbital Period (days) |\n",
    "|----------|-------------------------|-----------------------|\n",
    "| Mercury  | 57.59                   | 87.99                 |\n",
    "| Venus    | 108.11                  | 224.7                 |\n",
    "| Earth    | 149.57                  | 365.26                |\n",
    "| Mars     | 227.84                  | 686.98                |\n",
    "| Jupiter  | 778.14                  | 4332.4                |\n",
    "| Saturn   | 1427                    | 10759                 |\n",
    "| Uranus   | 2870.3                   | 30684                 |\n",
    "| Neptune  | 4499.9                   | 60188                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06838256",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 4: (5+5+5 points)\n",
    "\n",
    "1. Let $f(x) = c^T x$ where $x$ is an $n$-component vector. What is the relative **condition number** of $f$ (depending on $x$ and $c$) in the $L_1$ norm? in the $L_2$ norm?\n",
    "\n",
    "2. Suppose that you have two data points $(x_1, y_1)$ and $(x_2, y_2)$ and you linearly interpolate $y$ at a point $x$ ($x_1 \\leq x \\leq x_2$). If we think of $y(y_1, y_2)$ as a function of the input function values (keeping $x_1$ and $x_2$ fixed), show that the absolute **condition number** (in the $L_2$ norm) is bounded but the relative **condition number** can be infinite. Why does the absolute **condition number** make sense in this case?\n",
    "\n",
    "3. If $Q$ is a square matrix with orthonormal columns (so that $Q^T Q = I$), explain why its induced norm (as defined in class) and **condition number** are both 1 (in the $L_2$ norm). Such \"orthogonal\" (or \"unitary\") matrices are the best case for solving linear systems!  (This result also generalizes to \"tall\" matrices with orthonormal columns.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589aedbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
