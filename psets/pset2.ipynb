{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0bf8ab-99e0-4356-9624-fd0cdfe769a9",
   "metadata": {},
   "source": [
    "# Pset-2\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Consider a linear regression problem in which your input data $x$ lies within the narrow domain $[100000, 100001]$. You want to fit a model of the form:\n",
    "\n",
    "$$\n",
    "y \\approx \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "You can represent this model in two different bases:\n",
    "1. The **standard monomial basis**: $\\{1, x\\}$,\n",
    "2. A **shifted basis**: $\\{1, x - 100000.5\\}$.\n",
    "\n",
    "For $N$ data points $\\{(x_i, y_i)\\}_{i=1}^N$, the mean squared error (MSE) loss function for both is written as:\n",
    "\n",
    "$$\n",
    "L(\\beta_0, \\beta_1) = \\frac{1}{N} \\sum_{i=1}^N \\bigl(\\beta_0 + \\beta_1 x_i - y_i\\bigr)^2,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\tilde{L}(\\tilde{\\beta}_0, \\tilde{\\beta}_1) = \\frac{1}{N} \\sum_{i=1}^N \\Bigl(\\tilde{\\beta}_0 + \\tilde{\\beta}_1 (x_i - 100000.5) - y_i\\Bigr)^2.\n",
    "$$\n",
    "\n",
    "#### Tasks:\n",
    "- Compute the gradient of each loss function with respect to its parameters.\n",
    "- Suppose you perform **one step** of gradient descent for each basis, using the **same** learning rate and **same** initial parameter guesses. How much does the loss decrease in each case? Explain any observed differences.\n",
    "- A useful diagnostic in gradient-based methods is to examine how well the local gradient direction aligns with the true direction toward the optimum. Let $\\boldsymbol{\\beta}^*$ be the optimal parameter vector that minimizes the loss, and let $\\nabla L(\\boldsymbol{\\beta})$ denote the gradient of the loss at some iterate $\\boldsymbol{\\beta}$.\n",
    "\n",
    "  - **Compute or estimate** the angle between $\\boldsymbol{\\beta}^*$ and $\\nabla L(\\boldsymbol{\\beta})$ for each of the two bases:\n",
    "\n",
    "    $$\n",
    "       \\theta = \\cos^{-1}\\!\\Biggl(\n",
    "         \\frac{\\bigl\\langle \\boldsymbol{\\beta}^*, \\nabla L(\\boldsymbol{\\beta}) \\bigr\\rangle}\n",
    "         {\\|\\boldsymbol{\\beta}^*\\|\\;\\|\\nabla L(\\boldsymbol{\\beta})\\|}\n",
    "       \\Biggr).\n",
    "    $$\n",
    "\n",
    "  - How large is the angle in each case, and what does this imply about the alignment of the gradient direction toward the optimal solution? Relate your observations to the conditioning of each basis and the implications for gradient-descent convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "We know that if $f(x)$ is analytic in an open real interval containing $[-1,1]$, then there exist constants $C > 0$ and $K > 1$ such that:\n",
    "\n",
    "$$\n",
    "    \\max_{x \\in [-1,1]} |f(x) - p(x)| \\leq C K^{-n},\n",
    "$$\n",
    "\n",
    "where $p$ is the unique polynomial of degree $n$ or less defined by interpolation on $n + 1$ Chebyshev second-kind points. But now consider the family of functions $f_m(x) = |x|^m$.\n",
    "\n",
    "#### Tasks:\n",
    "- **Differentiability:** How many continuous derivatives over $[-1,1]$ does $f_m$ possess?\n",
    "- **Polynomial Interpolation:** Compute the polynomial interpolant using $n$ second-kind Chebyshev nodes in $[-1,1]$ for $n = 10, 20, 30, \\dots, 100$. At each value of $n$, compute the max-norm error:\n",
    "\n",
    "  $$\n",
    "      \\max |p(x) - f_m(x)|\n",
    "  $$\n",
    "\n",
    "  evaluated for at least 41000 values of $x$. Using a single log-log graph, plot the error as a function of $n$ for all six values $m = 1, 3, 5, 7, 9, 11$.\n",
    "\n",
    "- **Asymptotic Hypothesis:** Based on the results of parts (a) and (b), form a hypothesis about the asymptotic behavior of the error for fixed $m$ as $n \\to \\infty$.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Kepler found that the orbital period $\\tau$ of a planet depends on its mean distance $R$ from the sun according to:\n",
    "\n",
    "$$\n",
    "    \\tau = c R^{\\alpha}\n",
    "$$\n",
    "\n",
    "for a simple rational number $\\alpha$. Perform a linear least-squares fit from the following table in order to determine the most likely simple rational value of $\\alpha$.\n",
    "\n",
    "| Planet   | Distance from Sun (Mkm) | Orbital Period (days) |\n",
    "|----------|-------------------------|-----------------------|\n",
    "| Mercury  | 57.59                   | 87.99                 |\n",
    "| Venus    | 108.11                  | 224.7                 |\n",
    "| Earth    | 149.57                  | 365.26                |\n",
    "| Mars     | 227.84                  | 686.98                |\n",
    "| Jupiter  | 778.14                  | 4332.4                |\n",
    "| Saturn   | 1427                    | 10759                 |\n",
    "| Uranus   | 2870.3                   | 30684                 |\n",
    "| Neptune  | 4499.9                   | 60188                 |\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 4: Condition Number\n",
    "\n",
    "1. Let $f(x) = c^T x$ where $x$ is an $n$-component vector. What is are the relative **condition number** of $f$ (depending on $x$ and $c$) in the $L_1$ norm? in the $L_2$ norm?\n",
    "\n",
    "2. Suppose that you have two data points $(x_1, y_1)$ and $(x_2, y_2)$ and you linearly interpolate $y$ at a point $x$ ($x_1 \\leq x \\leq x_2$). If we think of $y(y_1, y_2)$ as a function of the input function values (keeping $x_1$ and $x_2$ fixed), show that the absolute **condition number** (in the $L_2$ norm) is bounded but the relative **condition number** can be infinite. Why does the absolute **condition number** make sense in this case?\n",
    "\n",
    "3. If $Q$ is a square matrix with orthonormal columns (so that $Q^T Q = I$), explain why its induced norm (defined in class) and **condition number** are both 1 (in the $L_2$ norm). Such \"orthogonal\" (or \"unitary\") matrices are the best case for solving linear systems!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b492d-7e6e-4b48-a366-058c394b5439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
