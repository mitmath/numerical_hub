{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b601bd-43e8-4d3e-b733-c8a3e6d58fe1",
   "metadata": {},
   "source": [
    "# 18.S190/6.S090 Problem Set 1 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dbef1",
   "metadata": {},
   "source": [
    "## Problem 1 (5+5+6+4 points)\n",
    "\n",
    "The [course notebook on finite differences](https://github.com/mitmath/numerical_hub/blob/fbcbf6adef724392624921c5a7cf8a9d53330347/notes/finite-differences.ipynb)\n",
    "includes, without derivation, a mysterious four-line Julia function\n",
    "called `stencil` that can compute finite-difference rules for\n",
    "an arbitrary number of points.  The `stencil` function is reproduced below, in both Julia and Python.\n",
    "\n",
    "In particular, if you want to compute\n",
    "the $m$-th derivative of a smooth (analytic) scalar function $f(x)$\n",
    "at $x_{0}$, it returns the weights $w_{k}$ of an $n$-point ($n>m$)\n",
    "finite-difference rule from evaluating $f$ at points $x_{k}$ for\n",
    "$k=1\\ldots n$:\n",
    "$$\n",
    "f^{(m)}(x_{0})\\approx\\sum_{k=1}^{n}w_{k}f(x_{k})\n",
    "$$\n",
    "by solving the system of equations $Aw=e_{m+1}$, where $e_{j}\\in\\mathbb{R}^{n}$\n",
    "is the Cartesian unit vector in the $j$-th direction and $A$ is\n",
    "an $n\\times n$ matrix with entries $A_{ij}=\\frac{(x_{j}-x_{0})^{i-1}}{(i-1)!}$ (where $i, j = 1, \\ldots, n$ are the rows and columns of $A$, respectively).\n",
    "\n",
    "Here, you will analyze and derive this technique.\n",
    "\n",
    "1. Let $x_{0}=0$. According to the notes, you can then compute $f^{(m)}(y)\\approx\\frac{1}{h^{m}}\\sum_{k=1}^{n}w_{k}f(y+hx_{k})$\n",
    "for an arbitrary point $y$ and an arbitrary step-size scaling factor\n",
    "$h$ (which can be made smaller and smaller to reduce truncation errors; i.e. $h=\\delta x$).\n",
    "Derive this formula from the $f^{(m)}(x_{0})\\approx \\cdots$ formula above (via the chain rule and a change of variables).\n",
    "\n",
    "2. Now evaluate it for $x_{0}=0$ (`0//1` in Julia for exact rational results)\n",
    "and $x=[0,1,2,3]$ with $m=1$, i.e. using $n=4$ equally spaced points\n",
    "$\\ge x_{0}$ (a *higher-order* \"forward-difference\" formula). Use the resulting weights, in the formula scaled by\n",
    "$h$ as above, to approximate the derivative $f'(1)$ for $f(x)=\\sin(x)$,\n",
    "and plot the relative error (compared to the exact derivative) as\n",
    "a function of $h$ on a log–log scale, similar to the course notebook.\n",
    "What power law in $h$ does the truncation error (approximately) seem\n",
    "to follow? That is, what is the “order of accuracy”?\n",
    "\n",
    "3. Derive the stencil equation $Aw=e_{m+1}$ above: write out the first\n",
    "$n$ terms of the Taylor series (up to the $f^{(n-1)}$ derivative) for $f(x_{0}+\\delta x)$, and try\n",
    "to find a linear combination of this series evaluated at $\\delta x=x_{k}-x_{0}$\n",
    "for $k=1\\ldots n$ in such a way that you obtain $f^{(m)}(x_{0})$.\n",
    "\n",
    "4. Explain the output of `stencil` for $x =[-1,+1]$, $x_{0}=0$ (or `0//1` in Julia for exact rational results), and $m=0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f84335",
   "metadata": {},
   "source": [
    "### Solutions:\n",
    "\n",
    "\n",
    "1. Consider the function $g(x) = f(y+hx)$.  By the chain rule, $g^{(m)}(x) = h^m f^{(m)}(y+hx)$, so it follows that $f^{(m)}(y) = \\frac{1}{h^m} g^{(m)}(0)$.  Since $x_0 = 0$, plug in the finite-difference formula for $g^{(m)}(0) \\approx \\sum_{k=1}^n w_k g(x_k) = \\sum_{k=1}^n w_k f(y+h x_k)$, and the result follows.\n",
    "\n",
    "    (The key thing to remember is that the finite-difference stencil is for *any* function, not just for functions called \"$f$\")\n",
    "   \n",
    "2. For $x=[0,1,2,3]$, the 'stencil' function returns $w = [-11/6, 3, -3/2, 1/3]$ (see Julia code below).  Trying it out numerically for $\\sin'(1)$, we find that the error scales as $\\boxed{\\sim h^3}$, i.e.~it is *third-order accurate*.\n",
    "\n",
    "    \n",
    "3.  The familiar Taylor series formula using $\\delta x = (x_j-x_0)$ takes the form\n",
    "$f(x_0 + (x_j-x_0) ) = f(x_j)   = \\sum_{i=1}^\\infty \\frac{f^{(i-1)}(x_0)}{ (i-1)! } (x_j -x_0)^{i-1}.$ Letting $A_{ij}=(x_j-x_0)^{i-1}/(i-1)!$, as suggested, we have that\n",
    "$f(x_j) = \\sum_{i=1}^\\infty A_{ij} f^{(i-1)}(x_0)$.  To form the approximation we truncate to $n$ terms, and write the equations in matrix form:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "f(x_1) \\\\ \\vdots \\\\ f(x_n) \n",
    "\\end{pmatrix}\n",
    "=\n",
    "A^T\n",
    "\\begin{pmatrix}\n",
    "f(x_0) \\\\ \\vdots \\\\ f^{(n-1)}(x_0) \n",
    "\\end{pmatrix} .\n",
    "$$\n",
    "Taking an inner product with $e_m$ and rearranging we see that\n",
    "$$ \\begin{pmatrix}\n",
    "f(x_1) \\\\ \\vdots \\\\ f(x_n) \n",
    "\\end{pmatrix}^T A^{-1}e_m = f^{(m)}(x_0)$$\n",
    "as desired, since the left hand side is exactly $\\sum_{k=1}^n w_k f(x_k)$\n",
    "\n",
    "4. The 'stencil' function returns $w = [1/2, 1/2]$ (see Julia code below). This is the interpolation of data at midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ece403-b9b0-4cbc-a8f1-0d0bdef797e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stencil (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Julia:\n",
    "function stencil(x::AbstractVector{<:Real}, x₀::Real, m::Integer)\n",
    "    ℓ = 0:length(x)-1\n",
    "    m in ℓ || throw(ArgumentError(\"invalid derivative order m\"))\n",
    "    A = @. (x' - x₀)^ℓ / factorial(ℓ)\n",
    "    return A \\ (ℓ .== m) # vector of weights w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68426622",
   "metadata": {},
   "source": [
    "#### part 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f24ad3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Rational{Int64}}:\n",
       " -11//6\n",
       "    3\n",
       "  -3//2\n",
       "   1//3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stencil(0:3, 0//1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693e079",
   "metadata": {},
   "source": [
    "#### part 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be73a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Rational{Int64}}:\n",
       " 1//2\n",
       " 1//2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stencil([-1, 1], 0//1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce0678-58f7-40dd-a30d-7908c24e90d8",
   "metadata": {},
   "source": [
    "## Problem 2 (4+4+4+8 points)\n",
    "\n",
    "Write a function `myexp(x)` (in Julia or Python) to compute $e^x$ directly from the Taylor series definition:\n",
    "$$\n",
    "e^x = 1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!} + \\cdots \\, .\n",
    "$$\n",
    "(in the default `Float64`/`float` precision … no fair using arbitrary-precision arithmetic).\n",
    "\n",
    "1. Explain how you can compute each term in the series from the preceding term.  (Not only is this more efficient, but it also helps avoid overflow compared to the naive approach where you compute $x^n$ and $n!$ *separately* and then divide them.)\n",
    "\n",
    "2. Explain how you decided how many terms to sum, to make reasonably sure that the omitted terms have a negligible contribution.  (Your method should depend on $x$.  Does not need a rigorous argument, just a reasonable explanation.)\n",
    "\n",
    "3. Check that `myexp(100.0)` gives a small *relative* error ($< 10^{-14}$) compared to `exp(100.0)` in Julia or `math.exp(100.0)` in Python), even though your `myexp(100.0) - exp(100.0)` is probably huge.\n",
    "\n",
    "4. Explain why `myexp(-100.0)` gives a completely wrong result, no matter how many terms you include in the sum!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db35ff",
   "metadata": {},
   "source": [
    "### Solutions:\n",
    "\n",
    "1. Each term is $x/n$ times the previous term\n",
    "2. The summation should stop when the |current term| is less than the machine epsilon times the |current sum|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ece6fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myexp (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function myexp(x; tol=eps(Float64))\n",
    "    cur_sum, cur_term, n = 1, x, 1\n",
    "    while abs(cur_term / cur_sum) > tol\n",
    "        cur_sum += cur_term\n",
    "        n += 1\n",
    "        cur_term *= x/n\n",
    "    end\n",
    "    return cur_sum\n",
    "end\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd8497",
   "metadata": {},
   "source": [
    "3. The difference is huge, but the relative error is indeed very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcdb20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.951760157141521e27, 1.842092399959933e-16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myexp(100) - exp(100), abs( (myexp(100) - exp(100)) / exp(100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0165227",
   "metadata": {},
   "source": [
    "4. The result is wrong due to cancellation error: the terms are large with alternating signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d02c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.144652745098074e25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myexp(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf6259-77ef-4696-8834-9225ab324447",
   "metadata": {},
   "source": [
    "## Problem 3 (5+5 points)\n",
    "\n",
    "Write a function $L4(x,y)$ in Julia or Python that computes the \"$L_4$ norm\" $L4(x,y) = (x^4 + y^4)^{1/4}$ of two real (floating-point) scalars $x$ and $y$.\n",
    "\n",
    "1. If you implement this in the most straightforward way, directly from the formula above, does your code give an accurate answer for `L4(1e-100, 0.0)`?  What about for `L4(1e100, 0.0)`?  Why or why not?\n",
    "2. *Fix* your code so that it gives an accurate answer (a *small relative error* close to machine precision)) for all floating-point inputs $x$ and $y$ (including the case in the previous part). (No fair resorting to higher-precision arithmetic!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8080e4",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d412a51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L4 (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L4(x,y) = (abs(x)^4 + abs(y)^4)^(1/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef55c3",
   "metadata": {},
   "source": [
    "1. We should have `L4(x,0)` give |x|, but for very small or very large `x` we get floating-point **underflow** or **overflow**, respectively. In the default double precision (`Float64`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78af5b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L4(1e-100, 0) # (1e-100)⁴ underflows to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e675b35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L4(1e+100, 0)  # (1e+100)⁴ overflows to Inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114695b",
   "metadata": {},
   "source": [
    "2. To eliminate this problem, we can simply compute $s = \\max\\{|x|,|y|\\}$ and then pull out this scale factor, since in exact arithmetic $L_4(x,y) = s L_4(x/s,y/s)$ for any $s > 0$.  In this way, we avoid underflow/overflow in the leading-order term.  (If $|y|\\ll |x|$ and $|y/x|^4$ underflows to zero, we don't care, because $1 \\oplus |y/x|^4$ will round to `1.0` long before that point.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039b980a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L4good (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function L4good(x,y)\n",
    "    ax, ay = abs(x), abs(y)\n",
    "    s = max(ax,ay)\n",
    "    if s == 0\n",
    "        return float(s) # don't divide by zero if x==y==0\n",
    "    else\n",
    "        return s * ((ax/s)^4 + (ay/s)^4)^(1/4)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020682e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0e-100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L4good(1e-100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c5cb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0e100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L4good(1e+100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78612bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L4good(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e233a",
   "metadata": {},
   "source": [
    "If we compute the maximum relative error (compared to BigFloat) for million random numbers with random magnitudes from 1$0^−308$ to $10^+308$, we can it is accurate to within a few ulps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2e1586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum relative err = 9.950714535858108e-16 = 4.481403427576075 ulps.\n"
     ]
    }
   ],
   "source": [
    "maxerr = 0.0\n",
    "for i = 1:10^6\n",
    "    x = (rand() - 0.5) * 10.0^rand(-308:308)\n",
    "    y = (rand() - 0.5) * 10.0^rand(-308:308)\n",
    "    result = L4good(x,y)\n",
    "    exact = L4good(big(x), big(y)) # in 256-bit precision by default\n",
    "    maxerr = max(maxerr, Float64(abs(result - exact) / abs(exact)))\n",
    "end\n",
    "println(\"maximum relative err = \", maxerr, \" = \", maxerr/eps(Float64), \" ulps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c17302-2712-4de4-888d-b2e0b1cdff64",
   "metadata": {},
   "source": [
    "## Problem 4 ((2+3)+(3+6) points)\n",
    "\n",
    "In this problem set, you will apply two interpolation methods to approximate the function:\n",
    "\n",
    "$$\n",
    "s(t) = t^3 - t^2 + 2t, \\quad t \\in [0,5].\n",
    "$$\n",
    "\n",
    "We will use two different grids for interpolation:\n",
    "\n",
    "- **Uniform Grid:** Equally spaced nodes over $[0,5]$.\n",
    "- **Non-Uniform Grid:** Unequally spaced nodes that resemble experimental measurements.\n",
    "\n",
    "The corresponding function values are provided below:\n",
    "\n",
    "| $t$ (Uniform) | $s(t)$ |\n",
    "|-------------------|------------|\n",
    "| 0  | 0  |\n",
    "| 1  | 2  |\n",
    "| 2  | 8  |\n",
    "| 3  | 24 |\n",
    "| 4  | 56 |\n",
    "| 5  | 110 |\n",
    "\n",
    "| $t $ (Non-Uniform) | $ s(t) $ |\n",
    "|----------------------|------------|\n",
    "| 0.0  | 0.000  |\n",
    "| 0.8  | 1.472  |\n",
    "| 2.1  | 9.051  |\n",
    "| 3.0  | 24.000 |\n",
    "| 4.8  | 97.152 |\n",
    "| 5.0  | 110.000 |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpolation Methods\n",
    "\n",
    "For this problem, you will write **your own implementation** (not just calling a pre-written library function) of the following two interpolation methods:\n",
    "\n",
    "1. **Variant A (Consecutive Interpolation):**  \n",
    "   On each subinterval $[t_i,t_{i+1}]$ (assuming that the $t_i$ are sorted in ascending order), the interpolant is the linear function connecting the two consecutive data points $(t_i,s(t_i))$ and $(t_{i+1},s(t_{i+1}))$. \n",
    "\n",
    "2. **Variant B (Inverse Distance Weighted Interpolation):**  \n",
    "   For an evaluation point $t$, compute the following ratio of sums over *all* the data points:\n",
    "\n",
    "   $$\n",
    "   s_B(t)= \\frac{\\sum_i \\frac{1}{|t-t_i|^2}\\, s(t_i)}\n",
    "   {\\sum_j \\frac{1}{|t-t_j|^2}}\n",
    "   $$\n",
    "\n",
    "   Note that as $t \\to t_i$ (one of the data points), $s_B(t) \\to s(t_i)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 1: Implementing and inspecting the interpolations\n",
    "\n",
    "**(a)** For evaluation points $t^*=1.5$ and $t^*=2.5$, compute the interpolated value using both Variant A and Variant B on the uniform grid. Repeat the interpolation for the non-uniform grid.  \n",
    "\n",
    "**(b)** Plot the two interpolants (Variant A and Variant B) for each grid, for a dense set of points $t \\in [0,5]$.  (Be careful when evaluating variant B exactly at a data point, so you don't divide `Inf` by `Inf`.) \n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Accuracy Analysis\n",
    "\n",
    "#### (a) Variant A\n",
    "\n",
    "Given that the true function is $ s(t)=t^3-t^2+2t $, for an arbitrary evaluation point $t_0$ in some subinterval $[t_i,t_{i+1}]$, perform a Taylor series expansion of $s(t)$ about $t_i$.  \n",
    "\n",
    "Using this Taylor series, show that the local interpolation error at any point $t_0$ can be expressed in terms of the following asymptotic upper bound (up to constant factors, i.e. \"big-O\" notation as in class):\n",
    "\n",
    "$$\n",
    "\\text{Error} = \\mathcal{O}((\\Delta t)^2),\n",
    "$$\n",
    "\n",
    "where  $\\Delta t = \\max \\{ t_{i+1}-t_i \\}$ (the maximum spacing), as $\\Delta t \\to 0$.  Does it matter if the points $t_i$ are uniform or nonuniform?\n",
    "\n",
    "#### (b) Variant B\n",
    "\n",
    "For the inverse distance weighted interpolation (Variant B), plot the error at $t = 2.5$ on a log–log scale as a function of the number of points $N$ for uniformly spaced points ($\\Delta t = 5/(N+1)$), for a sequence of *even* numbers $N$ (so that $t=2.5$ is not one of the data points), generating data from the formula $s(t)=t^3-t^2+2t$.\n",
    "\n",
    "What convergence rate do you observe, i.e. $\\mathcal{O}((\\Delta t)^2)$ or $\\mathcal{O}(\\Delta t)$ or ...?\n",
    "\n",
    "Prove this convergence rate, or at least try to make a convincing informal argument.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
